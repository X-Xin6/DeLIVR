{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOrVuQ1KApuNA7VMhCaeL/e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/X-Xin6/DeLIVR/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(IV, beta, true_g, gamma, seed = 0):\n",
        "\tif seed is None:\n",
        "\t\tseed = np.random.randint(0,2**32 - 1)\n",
        "\tnp.random.seed(seed)\n",
        "\ts1_size = IV.shape[0]//10\n",
        "\ts1_idx = np.random.choice(np.arange(IV.shape[0]), size = s1_size, replace = False)\n",
        "\ts2_idx = np.array(list(set(np.arange(IV.shape[0])).difference(s1_idx)))\n",
        "\ts1_IV = IV[s1_idx,:]\n",
        "\ts2_IV = IV[s2_idx,:]\n",
        "\t# stage 1 data\n",
        "\ts1_expr = (s1_IV@beta).squeeze() + np.random.normal(size = s1_size)\n",
        "\ts1_expr = s1_expr.reshape(-1,1)\n",
        "\t#\n",
        "\te = np.random.multivariate_normal(mean = [0,0], cov = [[1,gamma],[gamma,1]], size = s2_IV.shape[0])\n",
        "\ts2_expr = (s2_IV@beta).squeeze() + e[:,0]\n",
        "\ts2_pheno = true_g(s2_expr) + e[:,1]\n",
        "\ts2_expr = s2_expr.reshape(-1,1)\n",
        "\n",
        "\treturn s1_IV, s1_expr, s2_IV, s2_pheno"
      ],
      "metadata": {
        "id": "WDk_9HETJ2-_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn, optim\n",
        "#data simulation test\n",
        "#Data Simulation\n",
        "# Load data\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "simulated_IV = pd.read_csv('/content/drive/MyDrive/colab_data/simulated_IV.txt', sep=' ',header=None).values\n",
        "s1_beta = pd.read_csv('/content/drive/MyDrive/colab_data/s1_beta.txt', sep=' ',header=None).values.squeeze()\n",
        "true_g = lambda x: 3 * x ** 2  # Replace with your true function\n",
        "gamma = 0.7  # Replace with your gamma value\n",
        "s1_IV, s1_expr, s2_IV, s2_pheno = generate_data(simulated_IV, s1_beta, true_g, gamma)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LnzGcbGJqPn",
        "outputId": "91ee2fd3-c2a6-4ef5-f907-bb3c10a32ce3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_stage1(s1_IV, s1_expr, IV):\n",
        "    design = sm.add_constant(s1_IV)\n",
        "    stage1 = sm.OLS(s1_expr, design).fit()\n",
        "    s2_design = sm.add_constant(IV)\n",
        "    s2_expr = stage1.predict(s2_design).reshape(-1, 1)\n",
        "    return s2_expr\n",
        "def sample_split(expr, pheno, test_ratio, val_ratio, seed=None):\n",
        "    if seed is None:\n",
        "        seed = np.random.randint(0, 2**32 - 1)\n",
        "\n",
        "    # Split the data into training and validation + test sets\n",
        "    expr_train, expr_temp, pheno_train, pheno_temp = \\\n",
        "        train_test_split(expr, pheno, test_size=val_ratio + test_ratio, random_state=seed)\n",
        "\n",
        "    # Calculate the proportion of the test set relative to the combined validation and test sets\n",
        "    test_ratio_adjusted = test_ratio / (val_ratio + test_ratio)\n",
        "\n",
        "    # Split the validation + test sets into separate validation and test sets\n",
        "    expr_val, expr_test, pheno_val, pheno_test = \\\n",
        "        train_test_split(expr_temp, pheno_temp, test_size=test_ratio_adjusted, random_state=seed)\n",
        "\n",
        "    return expr_train, expr_val, expr_test, pheno_train, pheno_val, pheno_test\n",
        "\n",
        "\n",
        "def train_stage2(model, X_train, y_train, X_val, y_val, epochs, learning_rate):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Validation loss\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs, y_val)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "class Stage2Model(nn.Module):\n",
        "    def __init__(self, input_shape, layer_dims, l2):\n",
        "        super(Stage2Model, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(layer_dims)):\n",
        "            layers.append(nn.Linear(input_shape if i == 0 else layer_dims[i-1], layer_dims[i]))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.BatchNorm1d(layer_dims[i]))\n",
        "        layers.append(nn.Linear(layer_dims[-1], 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        self.regularization = l2\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def l2_regularization(self):\n",
        "        l2_reg = None\n",
        "        for W in self.model.parameters():\n",
        "            if l2_reg is None:\n",
        "                l2_reg = W.norm(2)\n",
        "            else:\n",
        "                l2_reg = l2_reg + W.norm(2)\n",
        "        return self.regularization * l2_reg\n"
      ],
      "metadata": {
        "id": "2XnspzFlLV6v"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JuKYoq3eIu8V",
        "outputId": "535b9c58-f930-47fe-a3d1-8910ce605c2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"if __name__ == '__main__':\\n    main()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "class DeLIVR:\n",
        "    def __init__(self, stage2_model):\n",
        "        self.stage2_model = stage2_model\n",
        "        self._expr = None\n",
        "        self._pheno = None\n",
        "        # Other necessary initializations\n",
        "\n",
        "    def sample_split(self, test_ratio, val_ratio, seed=None):\n",
        "        if seed is None:\n",
        "            seed = np.random.randint(0, 2**32 - 1)\n",
        "        if self._expr is None:\n",
        "            self.fit_stage1()\n",
        "        # Split the data into training, validation, and test sets\n",
        "        self._expr_train, self._expr_val, self._pheno_train, self._pheno_val = \\\n",
        "            train_test_split(self._expr, self._pheno, test_size=val_ratio, random_state=seed)\n",
        "        self._expr_train, self._expr_test, self._pheno_train, self._pheno_test = \\\n",
        "            train_test_split(self._expr_train, self._pheno_train, test_size=test_ratio/(1-val_ratio), random_state=seed)\n",
        "\n",
        "    def fit(self, s1_IV, s1_expr, s2_IV, s2_pheno, training_params, test_ratio, val_ratio):\n",
        "        # Fit the first stage\n",
        "        self._expr = self.fit_stage1(s1_IV, s1_expr, s2_IV)\n",
        "        self._pheno = s2_pheno\n",
        "\n",
        "        # Split the data\n",
        "        self.sample_split(test_ratio, val_ratio)\n",
        "\n",
        "        # Convert Stage 1 output to tensor for Stage 2\n",
        "        s2_expr_tensor = torch.tensor(self._expr_train, dtype=torch.float32)\n",
        "        s2_pheno_tensor = torch.tensor(self._pheno_train, dtype=torch.float32)\n",
        "\n",
        "        # Train the Stage 2 model (PyTorch)\n",
        "        # Assuming train_stage2 is a function that trains the Stage2Model\n",
        "        train_stage2(self.stage2_model, s2_expr_tensor, s2_pheno_tensor, training_params)\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Load data, initialize models, and other setup\n",
        "    # ...\n",
        "    s2_expr = fit_stage1(s1_IV, s1_expr, s2_IV)\n",
        "    print(s2_expr.shape)\n",
        "    test_ratio = 0.4\n",
        "    val_ratio = 0.1\n",
        "\n",
        "    expr_train, expr_val, expr_test, pheno_train, pheno_val, pheno_test = sample_split(s2_expr, s2_pheno, test_ratio, val_ratio)\n",
        "    expr_train_tensor = torch.tensor(expr_train, dtype=torch.float32)\n",
        "    expr_val_tensor = torch.tensor(expr_val, dtype=torch.float32)\n",
        "    expr_test_tensor = torch.tensor(expr_test, dtype=torch.float32)\n",
        "    pheno_train_tensor = torch.tensor(pheno_train, dtype=torch.float32)\n",
        "    pheno_val_tensor = torch.tensor(pheno_val, dtype=torch.float32)\n",
        "    pheno_test_tensor = torch.tensor(pheno_test, dtype=torch.float32)\n",
        "    stage2_model = Stage2Model(expr_train_tensor.shape[1], [32, 16, 8, 8, 8], 0.05)\n",
        "    train_stage2(stage2_model, expr_train_tensor, pheno_train_tensor, None, None)  # Assuming no validation set for simplicity\n",
        "\n",
        "\n",
        "'''if __name__ == '__main__':\n",
        "    main()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Stage2Model(nn.Module):\n",
        "    def __init__(self, input_shape, l2):\n",
        "        super(Stage2Model, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_shape, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Linear(16, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(8),\n",
        "            nn.Linear(8, 1)\n",
        "        )\n",
        "        self.regularization = l2\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def l2_regularization(self):\n",
        "        l2_reg = None\n",
        "        for W in self.model.parameters():\n",
        "            if l2_reg is None:\n",
        "                l2_reg = W.norm(2)\n",
        "            else:\n",
        "                l2_reg = l2_reg + W.norm(2)\n",
        "        return self.regularization * l2_reg\n"
      ],
      "metadata": {
        "id": "F14yOd3Qpy1-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    s2_expr = fit_stage1(s1_IV, s1_expr, s2_IV)\n",
        "    print(s2_expr.shape)\n",
        "    test_ratio = 0.4\n",
        "    val_ratio = 0.1\n",
        "\n",
        "    expr_train, expr_val, expr_test, pheno_train, pheno_val, pheno_test = sample_split(s2_expr, s2_pheno, test_ratio, val_ratio)\n",
        "    expr_train_tensor = torch.tensor(expr_train, dtype=torch.float32)\n",
        "    expr_val_tensor = torch.tensor(expr_val, dtype=torch.float32)\n",
        "    expr_test_tensor = torch.tensor(expr_test, dtype=torch.float32)\n",
        "    pheno_train_tensor = torch.tensor(pheno_train, dtype=torch.float32)\n",
        "    pheno_val_tensor = torch.tensor(pheno_val, dtype=torch.float32)\n",
        "    pheno_test_tensor = torch.tensor(pheno_test, dtype=torch.float32)\n",
        "    stage2_model = Stage2Model(expr_train_tensor.shape[1], 0.05)\n",
        "    #train_stage2(stage2_model, expr_train_tensor, pheno_train_tensor, None, None)  # Assuming no validation set for simplicity\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'train_losses' and 'val_losses' are obtained from the training function\n",
        "    train_losses, val_losses = train_stage2(stage2_model, expr_train_tensor, pheno_train_tensor,expr_val_tensor,  pheno_val_tensor,  epochs=800, learning_rate=0.03)\n",
        "\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w9hH8ifLVabt",
        "outputId": "1c29b621-0310-4609-f72e-e9625d596154"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9000, 1)\n",
            "Epoch 1/800, Train Loss: 1616.6763916015625, Val Loss: 1468.910888671875\n",
            "Epoch 2/800, Train Loss: 1613.14599609375, Val Loss: 1464.8233642578125\n",
            "Epoch 3/800, Train Loss: 1609.5186767578125, Val Loss: 1458.6561279296875\n",
            "Epoch 4/800, Train Loss: 1605.322021484375, Val Loss: 1451.4681396484375\n",
            "Epoch 5/800, Train Loss: 1600.546875, Val Loss: 1444.740234375\n",
            "Epoch 6/800, Train Loss: 1595.2388916015625, Val Loss: 1438.140625\n",
            "Epoch 7/800, Train Loss: 1589.3233642578125, Val Loss: 1431.105712890625\n",
            "Epoch 8/800, Train Loss: 1582.777587890625, Val Loss: 1423.645263671875\n",
            "Epoch 9/800, Train Loss: 1575.6026611328125, Val Loss: 1416.038818359375\n",
            "Epoch 10/800, Train Loss: 1567.8114013671875, Val Loss: 1408.481689453125\n",
            "Epoch 11/800, Train Loss: 1559.3895263671875, Val Loss: 1401.0074462890625\n",
            "Epoch 12/800, Train Loss: 1550.3271484375, Val Loss: 1393.408935546875\n",
            "Epoch 13/800, Train Loss: 1540.6279296875, Val Loss: 1385.4334716796875\n",
            "Epoch 14/800, Train Loss: 1530.2938232421875, Val Loss: 1376.647216796875\n",
            "Epoch 15/800, Train Loss: 1519.3275146484375, Val Loss: 1366.646728515625\n",
            "Epoch 16/800, Train Loss: 1507.73486328125, Val Loss: 1355.3289794921875\n",
            "Epoch 17/800, Train Loss: 1495.5257568359375, Val Loss: 1342.9063720703125\n",
            "Epoch 18/800, Train Loss: 1482.7188720703125, Val Loss: 1329.7994384765625\n",
            "Epoch 19/800, Train Loss: 1469.3399658203125, Val Loss: 1316.555419921875\n",
            "Epoch 20/800, Train Loss: 1455.421142578125, Val Loss: 1303.65087890625\n",
            "Epoch 21/800, Train Loss: 1440.9974365234375, Val Loss: 1291.402099609375\n",
            "Epoch 22/800, Train Loss: 1426.1075439453125, Val Loss: 1279.6436767578125\n",
            "Epoch 23/800, Train Loss: 1410.7962646484375, Val Loss: 1267.90673828125\n",
            "Epoch 24/800, Train Loss: 1395.1129150390625, Val Loss: 1255.7403564453125\n",
            "Epoch 25/800, Train Loss: 1379.1112060546875, Val Loss: 1242.923828125\n",
            "Epoch 26/800, Train Loss: 1362.850341796875, Val Loss: 1229.432373046875\n",
            "Epoch 27/800, Train Loss: 1346.398193359375, Val Loss: 1215.43115234375\n",
            "Epoch 28/800, Train Loss: 1329.832275390625, Val Loss: 1201.2366943359375\n",
            "Epoch 29/800, Train Loss: 1313.23681640625, Val Loss: 1187.3232421875\n",
            "Epoch 30/800, Train Loss: 1296.694580078125, Val Loss: 1174.0362548828125\n",
            "Epoch 31/800, Train Loss: 1280.286865234375, Val Loss: 1161.2938232421875\n",
            "Epoch 32/800, Train Loss: 1264.1070556640625, Val Loss: 1148.973876953125\n",
            "Epoch 33/800, Train Loss: 1248.257568359375, Val Loss: 1137.0875244140625\n",
            "Epoch 34/800, Train Loss: 1232.8397216796875, Val Loss: 1125.7462158203125\n",
            "Epoch 35/800, Train Loss: 1217.9520263671875, Val Loss: 1115.1612548828125\n",
            "Epoch 36/800, Train Loss: 1203.691162109375, Val Loss: 1105.4552001953125\n",
            "Epoch 37/800, Train Loss: 1190.1544189453125, Val Loss: 1096.611328125\n",
            "Epoch 38/800, Train Loss: 1177.437744140625, Val Loss: 1088.528076171875\n",
            "Epoch 39/800, Train Loss: 1165.6324462890625, Val Loss: 1081.1055908203125\n",
            "Epoch 40/800, Train Loss: 1154.8148193359375, Val Loss: 1074.426025390625\n",
            "Epoch 41/800, Train Loss: 1145.0489501953125, Val Loss: 1068.7470703125\n",
            "Epoch 42/800, Train Loss: 1136.38916015625, Val Loss: 1064.3134765625\n",
            "Epoch 43/800, Train Loss: 1128.87548828125, Val Loss: 1061.1898193359375\n",
            "Epoch 44/800, Train Loss: 1122.523193359375, Val Loss: 1059.2508544921875\n",
            "Epoch 45/800, Train Loss: 1117.3211669921875, Val Loss: 1058.298828125\n",
            "Epoch 46/800, Train Loss: 1113.238525390625, Val Loss: 1058.198974609375\n",
            "Epoch 47/800, Train Loss: 1110.2205810546875, Val Loss: 1058.885498046875\n",
            "Epoch 48/800, Train Loss: 1108.1864013671875, Val Loss: 1060.2828369140625\n",
            "Epoch 49/800, Train Loss: 1107.02978515625, Val Loss: 1062.263671875\n",
            "Epoch 50/800, Train Loss: 1106.6231689453125, Val Loss: 1064.69482421875\n",
            "Epoch 51/800, Train Loss: 1106.8280029296875, Val Loss: 1067.43896484375\n",
            "Epoch 52/800, Train Loss: 1107.4952392578125, Val Loss: 1070.30712890625\n",
            "Epoch 53/800, Train Loss: 1108.47119140625, Val Loss: 1073.0439453125\n",
            "Epoch 54/800, Train Loss: 1109.6112060546875, Val Loss: 1075.41015625\n",
            "Epoch 55/800, Train Loss: 1110.7845458984375, Val Loss: 1077.312744140625\n",
            "Epoch 56/800, Train Loss: 1111.8797607421875, Val Loss: 1078.8358154296875\n",
            "Epoch 57/800, Train Loss: 1112.8079833984375, Val Loss: 1080.061767578125\n",
            "Epoch 58/800, Train Loss: 1113.50830078125, Val Loss: 1080.9178466796875\n",
            "Epoch 59/800, Train Loss: 1113.9471435546875, Val Loss: 1081.249755859375\n",
            "Epoch 60/800, Train Loss: 1114.1146240234375, Val Loss: 1081.008544921875\n",
            "Epoch 61/800, Train Loss: 1114.0230712890625, Val Loss: 1080.31103515625\n",
            "Epoch 62/800, Train Loss: 1113.701904296875, Val Loss: 1079.28662109375\n",
            "Epoch 63/800, Train Loss: 1113.19140625, Val Loss: 1077.9981689453125\n",
            "Epoch 64/800, Train Loss: 1112.5396728515625, Val Loss: 1076.5101318359375\n",
            "Epoch 65/800, Train Loss: 1111.79638671875, Val Loss: 1074.9552001953125\n",
            "Epoch 66/800, Train Loss: 1111.01025390625, Val Loss: 1073.4510498046875\n",
            "Epoch 67/800, Train Loss: 1110.224365234375, Val Loss: 1071.9993896484375\n",
            "Epoch 68/800, Train Loss: 1109.4755859375, Val Loss: 1070.5260009765625\n",
            "Epoch 69/800, Train Loss: 1108.7926025390625, Val Loss: 1069.0145263671875\n",
            "Epoch 70/800, Train Loss: 1108.1953125, Val Loss: 1067.5467529296875\n",
            "Epoch 71/800, Train Loss: 1107.695556640625, Val Loss: 1066.21875\n",
            "Epoch 72/800, Train Loss: 1107.298095703125, Val Loss: 1065.06494140625\n",
            "Epoch 73/800, Train Loss: 1107.0008544921875, Val Loss: 1064.0712890625\n",
            "Epoch 74/800, Train Loss: 1106.7967529296875, Val Loss: 1063.22216796875\n",
            "Epoch 75/800, Train Loss: 1106.6748046875, Val Loss: 1062.510009765625\n",
            "Epoch 76/800, Train Loss: 1106.621826171875, Val Loss: 1061.9176025390625\n",
            "Epoch 77/800, Train Loss: 1106.6236572265625, Val Loss: 1061.41796875\n",
            "Epoch 78/800, Train Loss: 1106.6654052734375, Val Loss: 1060.9932861328125\n",
            "Epoch 79/800, Train Loss: 1106.7333984375, Val Loss: 1060.641845703125\n",
            "Epoch 80/800, Train Loss: 1106.8150634765625, Val Loss: 1060.3681640625\n",
            "Epoch 81/800, Train Loss: 1106.899658203125, Val Loss: 1060.1693115234375\n",
            "Epoch 82/800, Train Loss: 1106.978515625, Val Loss: 1060.0323486328125\n",
            "Epoch 83/800, Train Loss: 1107.0452880859375, Val Loss: 1059.941162109375\n",
            "Epoch 84/800, Train Loss: 1107.0958251953125, Val Loss: 1059.8856201171875\n",
            "Epoch 85/800, Train Loss: 1107.12744140625, Val Loss: 1059.8646240234375\n",
            "Epoch 86/800, Train Loss: 1107.1400146484375, Val Loss: 1059.87890625\n",
            "Epoch 87/800, Train Loss: 1107.1339111328125, Val Loss: 1059.925537109375\n",
            "Epoch 88/800, Train Loss: 1107.111328125, Val Loss: 1059.99951171875\n",
            "Epoch 89/800, Train Loss: 1107.075439453125, Val Loss: 1060.099609375\n",
            "Epoch 90/800, Train Loss: 1107.029296875, Val Loss: 1060.2283935546875\n",
            "Epoch 91/800, Train Loss: 1106.9761962890625, Val Loss: 1060.38525390625\n",
            "Epoch 92/800, Train Loss: 1106.919677734375, Val Loss: 1060.561279296875\n",
            "Epoch 93/800, Train Loss: 1106.86328125, Val Loss: 1060.7467041015625\n",
            "Epoch 94/800, Train Loss: 1106.8094482421875, Val Loss: 1060.941162109375\n",
            "Epoch 95/800, Train Loss: 1106.760498046875, Val Loss: 1061.1500244140625\n",
            "Epoch 96/800, Train Loss: 1106.718017578125, Val Loss: 1061.3720703125\n",
            "Epoch 97/800, Train Loss: 1106.682861328125, Val Loss: 1061.598388671875\n",
            "Epoch 98/800, Train Loss: 1106.655517578125, Val Loss: 1061.8243408203125\n",
            "Epoch 99/800, Train Loss: 1106.635986328125, Val Loss: 1062.05224609375\n",
            "Epoch 100/800, Train Loss: 1106.623779296875, Val Loss: 1062.2764892578125\n",
            "Epoch 101/800, Train Loss: 1106.61767578125, Val Loss: 1062.482666015625\n",
            "Epoch 102/800, Train Loss: 1106.61669921875, Val Loss: 1062.6602783203125\n",
            "Epoch 103/800, Train Loss: 1106.61962890625, Val Loss: 1062.8131103515625\n",
            "Epoch 104/800, Train Loss: 1106.6253662109375, Val Loss: 1062.9481201171875\n",
            "Epoch 105/800, Train Loss: 1106.6322021484375, Val Loss: 1063.065185546875\n",
            "Epoch 106/800, Train Loss: 1106.6396484375, Val Loss: 1063.16015625\n",
            "Epoch 107/800, Train Loss: 1106.6466064453125, Val Loss: 1063.2340087890625\n",
            "Epoch 108/800, Train Loss: 1106.6522216796875, Val Loss: 1063.28759765625\n",
            "Epoch 109/800, Train Loss: 1106.656494140625, Val Loss: 1063.317138671875\n",
            "Epoch 110/800, Train Loss: 1106.65869140625, Val Loss: 1063.320068359375\n",
            "Epoch 111/800, Train Loss: 1106.6590576171875, Val Loss: 1063.3016357421875\n",
            "Epoch 112/800, Train Loss: 1106.6578369140625, Val Loss: 1063.2701416015625\n",
            "Epoch 113/800, Train Loss: 1106.6551513671875, Val Loss: 1063.2283935546875\n",
            "Epoch 114/800, Train Loss: 1106.6512451171875, Val Loss: 1063.1737060546875\n",
            "Epoch 115/800, Train Loss: 1106.6466064453125, Val Loss: 1063.1048583984375\n",
            "Epoch 116/800, Train Loss: 1106.6416015625, Val Loss: 1063.0252685546875\n",
            "Epoch 117/800, Train Loss: 1106.6365966796875, Val Loss: 1062.9404296875\n",
            "Epoch 118/800, Train Loss: 1106.6318359375, Val Loss: 1062.853759765625\n",
            "Epoch 119/800, Train Loss: 1106.62744140625, Val Loss: 1062.7681884765625\n",
            "Epoch 120/800, Train Loss: 1106.6239013671875, Val Loss: 1062.6866455078125\n",
            "Epoch 121/800, Train Loss: 1106.6209716796875, Val Loss: 1062.61083984375\n",
            "Epoch 122/800, Train Loss: 1106.6187744140625, Val Loss: 1062.5386962890625\n",
            "Epoch 123/800, Train Loss: 1106.617431640625, Val Loss: 1062.4677734375\n",
            "Epoch 124/800, Train Loss: 1106.61669921875, Val Loss: 1062.40087890625\n",
            "Epoch 125/800, Train Loss: 1106.6165771484375, Val Loss: 1062.341796875\n",
            "Epoch 126/800, Train Loss: 1106.61669921875, Val Loss: 1062.2919921875\n",
            "Epoch 127/800, Train Loss: 1106.6173095703125, Val Loss: 1062.2496337890625\n",
            "Epoch 128/800, Train Loss: 1106.6180419921875, Val Loss: 1062.2144775390625\n",
            "Epoch 129/800, Train Loss: 1106.618896484375, Val Loss: 1062.1883544921875\n",
            "Epoch 130/800, Train Loss: 1106.6195068359375, Val Loss: 1062.1702880859375\n",
            "Epoch 131/800, Train Loss: 1106.619873046875, Val Loss: 1062.1573486328125\n",
            "Epoch 132/800, Train Loss: 1106.620361328125, Val Loss: 1062.14990234375\n",
            "Epoch 133/800, Train Loss: 1106.6207275390625, Val Loss: 1062.1494140625\n",
            "Epoch 134/800, Train Loss: 1106.62060546875, Val Loss: 1062.1556396484375\n",
            "Epoch 135/800, Train Loss: 1106.6204833984375, Val Loss: 1062.16552734375\n",
            "Epoch 136/800, Train Loss: 1106.6202392578125, Val Loss: 1062.1793212890625\n",
            "Epoch 137/800, Train Loss: 1106.6197509765625, Val Loss: 1062.1973876953125\n",
            "Epoch 138/800, Train Loss: 1106.6192626953125, Val Loss: 1062.2183837890625\n",
            "Epoch 139/800, Train Loss: 1106.61865234375, Val Loss: 1062.240966796875\n",
            "Epoch 140/800, Train Loss: 1106.6182861328125, Val Loss: 1062.2650146484375\n",
            "Epoch 141/800, Train Loss: 1106.61767578125, Val Loss: 1062.2911376953125\n",
            "Epoch 142/800, Train Loss: 1106.6171875, Val Loss: 1062.3182373046875\n",
            "Epoch 143/800, Train Loss: 1106.6171875, Val Loss: 1062.3443603515625\n",
            "Epoch 144/800, Train Loss: 1106.6168212890625, Val Loss: 1062.36865234375\n",
            "Epoch 145/800, Train Loss: 1106.6165771484375, Val Loss: 1062.391357421875\n",
            "Epoch 146/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4112548828125\n",
            "Epoch 147/800, Train Loss: 1106.6165771484375, Val Loss: 1062.42919921875\n",
            "Epoch 148/800, Train Loss: 1106.6165771484375, Val Loss: 1062.44580078125\n",
            "Epoch 149/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4608154296875\n",
            "Epoch 150/800, Train Loss: 1106.6165771484375, Val Loss: 1062.473388671875\n",
            "Epoch 151/800, Train Loss: 1106.61669921875, Val Loss: 1062.4830322265625\n",
            "Epoch 152/800, Train Loss: 1106.6168212890625, Val Loss: 1062.489501953125\n",
            "Epoch 153/800, Train Loss: 1106.616943359375, Val Loss: 1062.4932861328125\n",
            "Epoch 154/800, Train Loss: 1106.6168212890625, Val Loss: 1062.49462890625\n",
            "Epoch 155/800, Train Loss: 1106.6171875, Val Loss: 1062.493896484375\n",
            "Epoch 156/800, Train Loss: 1106.616943359375, Val Loss: 1062.491455078125\n",
            "Epoch 157/800, Train Loss: 1106.6170654296875, Val Loss: 1062.4876708984375\n",
            "Epoch 158/800, Train Loss: 1106.6168212890625, Val Loss: 1062.481201171875\n",
            "Epoch 159/800, Train Loss: 1106.6168212890625, Val Loss: 1062.47314453125\n",
            "Epoch 160/800, Train Loss: 1106.6168212890625, Val Loss: 1062.46533203125\n",
            "Epoch 161/800, Train Loss: 1106.61669921875, Val Loss: 1062.4576416015625\n",
            "Epoch 162/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4488525390625\n",
            "Epoch 163/800, Train Loss: 1106.6165771484375, Val Loss: 1062.440185546875\n",
            "Epoch 164/800, Train Loss: 1106.61669921875, Val Loss: 1062.431884765625\n",
            "Epoch 165/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4229736328125\n",
            "Epoch 166/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4140625\n",
            "Epoch 167/800, Train Loss: 1106.6165771484375, Val Loss: 1062.40673828125\n",
            "Epoch 168/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4005126953125\n",
            "Epoch 169/800, Train Loss: 1106.6165771484375, Val Loss: 1062.3946533203125\n",
            "Epoch 170/800, Train Loss: 1106.616455078125, Val Loss: 1062.3895263671875\n",
            "Epoch 171/800, Train Loss: 1106.6165771484375, Val Loss: 1062.385986328125\n",
            "Epoch 172/800, Train Loss: 1106.6165771484375, Val Loss: 1062.3829345703125\n",
            "Epoch 173/800, Train Loss: 1106.6165771484375, Val Loss: 1062.3809814453125\n",
            "Epoch 174/800, Train Loss: 1106.6165771484375, Val Loss: 1062.3797607421875\n",
            "Epoch 175/800, Train Loss: 1106.6165771484375, Val Loss: 1062.3797607421875\n",
            "Epoch 176/800, Train Loss: 1106.6165771484375, Val Loss: 1062.380126953125\n",
            "Epoch 177/800, Train Loss: 1106.6165771484375, Val Loss: 1062.3812255859375\n",
            "Epoch 178/800, Train Loss: 1106.6165771484375, Val Loss: 1062.3829345703125\n",
            "Epoch 179/800, Train Loss: 1106.6165771484375, Val Loss: 1062.385009765625\n",
            "Epoch 180/800, Train Loss: 1106.616455078125, Val Loss: 1062.3873291015625\n",
            "Epoch 181/800, Train Loss: 1106.6165771484375, Val Loss: 1062.3902587890625\n",
            "Epoch 182/800, Train Loss: 1106.6165771484375, Val Loss: 1062.3931884765625\n",
            "Epoch 183/800, Train Loss: 1106.6165771484375, Val Loss: 1062.3961181640625\n",
            "Epoch 184/800, Train Loss: 1106.616455078125, Val Loss: 1062.399169921875\n",
            "Epoch 185/800, Train Loss: 1106.6165771484375, Val Loss: 1062.402099609375\n",
            "Epoch 186/800, Train Loss: 1106.616455078125, Val Loss: 1062.40478515625\n",
            "Epoch 187/800, Train Loss: 1106.616455078125, Val Loss: 1062.406982421875\n",
            "Epoch 188/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4093017578125\n",
            "Epoch 189/800, Train Loss: 1106.6165771484375, Val Loss: 1062.41162109375\n",
            "Epoch 190/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4134521484375\n",
            "Epoch 191/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4146728515625\n",
            "Epoch 192/800, Train Loss: 1106.616455078125, Val Loss: 1062.4156494140625\n",
            "Epoch 193/800, Train Loss: 1106.6165771484375, Val Loss: 1062.41650390625\n",
            "Epoch 194/800, Train Loss: 1106.6165771484375, Val Loss: 1062.416748046875\n",
            "Epoch 195/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4168701171875\n",
            "Epoch 196/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4171142578125\n",
            "Epoch 197/800, Train Loss: 1106.6165771484375, Val Loss: 1062.41650390625\n",
            "Epoch 198/800, Train Loss: 1106.6165771484375, Val Loss: 1062.41552734375\n",
            "Epoch 199/800, Train Loss: 1106.616455078125, Val Loss: 1062.414794921875\n",
            "Epoch 200/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4140625\n",
            "Epoch 201/800, Train Loss: 1106.616455078125, Val Loss: 1062.4130859375\n",
            "Epoch 202/800, Train Loss: 1106.616455078125, Val Loss: 1062.4119873046875\n",
            "Epoch 203/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4110107421875\n",
            "Epoch 204/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4095458984375\n",
            "Epoch 205/800, Train Loss: 1106.616455078125, Val Loss: 1062.4083251953125\n",
            "Epoch 206/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4075927734375\n",
            "Epoch 207/800, Train Loss: 1106.6165771484375, Val Loss: 1062.406982421875\n",
            "Epoch 208/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4058837890625\n",
            "Epoch 209/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4053955078125\n",
            "Epoch 210/800, Train Loss: 1106.616455078125, Val Loss: 1062.4049072265625\n",
            "Epoch 211/800, Train Loss: 1106.616455078125, Val Loss: 1062.4044189453125\n",
            "Epoch 212/800, Train Loss: 1106.6165771484375, Val Loss: 1062.404052734375\n",
            "Epoch 213/800, Train Loss: 1106.616455078125, Val Loss: 1062.4039306640625\n",
            "Epoch 214/800, Train Loss: 1106.616455078125, Val Loss: 1062.404052734375\n",
            "Epoch 215/800, Train Loss: 1106.6165771484375, Val Loss: 1062.404052734375\n",
            "Epoch 216/800, Train Loss: 1106.6165771484375, Val Loss: 1062.404052734375\n",
            "Epoch 217/800, Train Loss: 1106.6165771484375, Val Loss: 1062.404541015625\n",
            "Epoch 218/800, Train Loss: 1106.616455078125, Val Loss: 1062.4049072265625\n",
            "Epoch 219/800, Train Loss: 1106.616455078125, Val Loss: 1062.405029296875\n",
            "Epoch 220/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4058837890625\n",
            "Epoch 221/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4066162109375\n",
            "Epoch 222/800, Train Loss: 1106.616455078125, Val Loss: 1062.40673828125\n",
            "Epoch 223/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4072265625\n",
            "Epoch 224/800, Train Loss: 1106.616455078125, Val Loss: 1062.4078369140625\n",
            "Epoch 225/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4083251953125\n",
            "Epoch 226/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4085693359375\n",
            "Epoch 227/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4090576171875\n",
            "Epoch 228/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4093017578125\n",
            "Epoch 229/800, Train Loss: 1106.6165771484375, Val Loss: 1062.409423828125\n",
            "Epoch 230/800, Train Loss: 1106.616455078125, Val Loss: 1062.40966796875\n",
            "Epoch 231/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4100341796875\n",
            "Epoch 232/800, Train Loss: 1106.6165771484375, Val Loss: 1062.41015625\n",
            "Epoch 233/800, Train Loss: 1106.6165771484375, Val Loss: 1062.409912109375\n",
            "Epoch 234/800, Train Loss: 1106.6165771484375, Val Loss: 1062.410400390625\n",
            "Epoch 235/800, Train Loss: 1106.6165771484375, Val Loss: 1062.410400390625\n",
            "Epoch 236/800, Train Loss: 1106.6165771484375, Val Loss: 1062.410400390625\n",
            "Epoch 237/800, Train Loss: 1106.6165771484375, Val Loss: 1062.410400390625\n",
            "Epoch 238/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4100341796875\n",
            "Epoch 239/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4100341796875\n",
            "Epoch 240/800, Train Loss: 1106.616455078125, Val Loss: 1062.409912109375\n",
            "Epoch 241/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4095458984375\n",
            "Epoch 242/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4095458984375\n",
            "Epoch 243/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4093017578125\n",
            "Epoch 244/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4088134765625\n",
            "Epoch 245/800, Train Loss: 1106.6165771484375, Val Loss: 1062.408935546875\n",
            "Epoch 246/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4085693359375\n",
            "Epoch 247/800, Train Loss: 1106.6165771484375, Val Loss: 1062.408203125\n",
            "Epoch 248/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4085693359375\n",
            "Epoch 249/800, Train Loss: 1106.6165771484375, Val Loss: 1062.408203125\n",
            "Epoch 250/800, Train Loss: 1106.616455078125, Val Loss: 1062.408203125\n",
            "Epoch 251/800, Train Loss: 1106.6165771484375, Val Loss: 1062.4080810546875\n",
            "Epoch 252/800, Train Loss: 1106.6165771484375, Val Loss: 1062.407958984375\n",
            "Epoch 253/800, Train Loss: 1106.6165771484375, Val Loss: 1062.408203125\n",
            "Epoch 254/800, Train Loss: 1106.616455078125, Val Loss: 1062.4078369140625\n",
            "Epoch 255/800, Train Loss: 1106.616455078125, Val Loss: 1062.40771484375\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-82ac007d6a7d>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Assuming 'train_losses' and 'val_losses' are obtained from the training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_stage2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage2_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpheno_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexpr_val_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpheno_val_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-45e2f4042739>\u001b[0m in \u001b[0;36mtrain_stage2\u001b[0;34m(model, X_train, y_train, X_val, y_val, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3328\u001b[0m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3329\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for para in stage2_model.parameters():\n",
        "  print(para)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgdEGkqPVmBZ",
        "outputId": "a21dde19-7b70-457e-b90f-e4689a792eb7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-1.0407],\n",
            "        [-0.9690],\n",
            "        [ 0.7116],\n",
            "        [ 0.8942],\n",
            "        [ 0.8285],\n",
            "        [-0.5318],\n",
            "        [ 0.2359],\n",
            "        [-0.9671],\n",
            "        [ 0.5425],\n",
            "        [ 0.5251],\n",
            "        [ 0.2022],\n",
            "        [-0.0817],\n",
            "        [ 0.1086],\n",
            "        [-0.1528],\n",
            "        [-0.2686],\n",
            "        [ 0.1577]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.3651,  0.2530, -0.8567, -0.3575,  0.6315, -0.1401, -0.0534, -0.3039,\n",
            "        -0.6587, -0.6996,  0.7123, -0.4376,  0.9671, -0.6866, -0.6916, -0.4930],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([1.1422, 1.0340, 1.0094, 0.7540, 1.1392, 1.3022, 0.6045, 0.8668, 0.9600,\n",
            "        1.3795, 0.7219, 1.0000, 0.8319, 1.0000, 1.0000, 1.0627],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.2883, -0.0954, -0.2069, -0.2258,  0.4022, -0.0184, -0.2117,  0.1574,\n",
            "         0.2289, -0.1249,  0.3003,  0.2108, -0.2114,  0.1230,  0.1939,  0.4543],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.2364,  0.0744,  0.0556, -0.0070, -0.2018,  0.4931, -0.0584,  0.3454,\n",
            "         -0.2291, -0.2767, -0.0571, -0.6427,  0.1057, -0.5837, -0.5996, -0.2732],\n",
            "        [-0.1265, -0.2266,  0.4458,  0.2876,  0.3054,  0.0013,  0.4334, -0.1298,\n",
            "         -0.0407,  0.4458,  0.0198, -0.2646,  0.2012,  0.0356, -0.2456,  0.0347],\n",
            "        [-0.4155, -0.3055, -0.4302, -0.0867,  0.2308, -0.0178, -0.1542, -0.3356,\n",
            "         -0.3199, -0.5982,  0.2575,  0.0546,  0.2842,  0.0609,  0.2644, -0.2211],\n",
            "        [ 0.1547,  0.2901, -0.1973, -0.0503, -0.3427,  0.5162, -0.0230,  0.4024,\n",
            "          0.1129, -0.0501, -0.3416, -0.1746, -0.1218,  0.1186,  0.0368, -0.0500],\n",
            "        [ 0.0110, -0.1156,  0.5709,  0.4761,  0.4300, -0.0413,  0.0898, -0.0584,\n",
            "          0.5234,  0.4158,  0.2482, -0.0624,  0.3078, -0.1154,  0.0247,  0.1777],\n",
            "        [-0.0311, -0.1761,  0.3396,  0.0366,  0.5903, -0.4044,  0.0740,  0.0054,\n",
            "          0.1614,  0.0926,  0.4058,  0.2412,  0.3839,  0.2020,  0.3609,  0.3507],\n",
            "        [-0.0969, -0.0193,  0.2453,  0.0880, -0.0155, -0.1356,  0.0590,  0.1511,\n",
            "          0.1836,  0.3760, -0.1167, -0.4121, -0.0381, -0.2395, -0.3626,  0.2893],\n",
            "        [ 0.4710,  0.3971, -0.1747, -0.2716, -0.0915,  0.2866, -0.3510,  0.0672,\n",
            "         -0.2392,  0.0395, -0.0746, -0.1888, -0.3821,  0.1958,  0.2185, -0.1524]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.2718,  0.0156, -0.1558, -0.1250, -0.1932,  0.2753, -0.4344,  0.0104],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.9204, 0.8616, 0.5158, 0.6173, 0.9512, 0.9994, 0.6764, 0.6513],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-1.5416, -1.5748,  1.5923,  1.5409,  1.5893,  1.5541, -1.5462,  1.5849],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-1.7891, -1.6863,  1.5330,  1.8214,  1.6182,  1.7528, -1.7675,  1.6350]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([1.1554], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, item in enumerate(stage2_model.parameters(),start=0):\n",
        "  print(index,item.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eqj1DPUY4oM",
        "outputId": "6ca46848-995c-4e5a-c79a-e5a9a46d8180"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([16, 1])\n",
            "1 torch.Size([16])\n",
            "2 torch.Size([16])\n",
            "3 torch.Size([16])\n",
            "4 torch.Size([8, 16])\n",
            "5 torch.Size([8])\n",
            "6 torch.Size([8])\n",
            "7 torch.Size([8])\n",
            "8 torch.Size([1, 8])\n",
            "9 torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming expr_test_tensor and pheno_test_tensor are your test datasets\n",
        "\n",
        "# Make predictions\n",
        "stage2_model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = stage2_model(expr_test_tensor).squeeze()\n",
        "\n",
        "# Convert predictions and actual values to NumPy for evaluation\n",
        "predictions_np = predictions.numpy()\n",
        "actual_np = pheno_test_tensor.numpy()\n",
        "\n",
        "# Calculate MSE and other metrics\n",
        "mse = np.mean((predictions_np - actual_np) ** 2)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = np.mean(np.abs(predictions_np - actual_np))\n",
        "\n",
        "print(f\"MSE: {mse}, RMSE: {rmse}, MAE: {mae}\")\n",
        "\n",
        "# Plot predictions vs actual values\n",
        "plt.scatter(expr_test_tensor.numpy(), actual_np, label=\"Actual\")\n",
        "plt.scatter(expr_test_tensor.numpy(), predictions_np, label=\"Predicted\", color=\"r\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "l7wLDy1caexR",
        "outputId": "1e1548a3-ba63-4230-c423-597af2e44a1d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1049.7342529296875, RMSE: 32.399600982666016, MAE: 22.289005279541016\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABg4klEQVR4nO3deXhTZdo/8G+StulCk1JKm1YKVEShlkWQpeIoS4ECMig4M6AoOrz4ikUR1EHmlUFERR1nxBXUHwIziLiLgOKwCAgWQRC1VhmoZRGaAgWSUuiWnN8fIaFps5yTnCQnyfdzXbmgyck5T07anDvPcz/3oxIEQQARERGRgqhD3QAiIiKi5higEBERkeIwQCEiIiLFYYBCREREisMAhYiIiBSHAQoREREpDgMUIiIiUhwGKERERKQ4MaFugC+sViuOHz+O5ORkqFSqUDeHiIiIRBAEAdXV1cjKyoJa7bmPJCwDlOPHjyM7OzvUzSAiIiIfHD16FO3atfO4TVgGKMnJyQBsL1Cn04W4NURERCSG2WxGdna24zruSVgGKPZhHZ1OxwCFiIgozIhJz2CSLBERESkOAxQiIiJSHAYoREREpDhhmYMihiAIaGxshMViCXVTyEexsbHQaDShbgYREYVARAYo9fX1qKiowPnz50PdFPKDSqVCu3bt0KpVq1A3hYiIgiziAhSr1Yry8nJoNBpkZWUhLi6OxdzCkCAIOHnyJH777Td07tyZPSlERFEm4gKU+vp6WK1WZGdnIzExMdTNIT+0bdsWhw4dQkNDAwMUIqIoE7FJst5K6JLyseeLiCh6RVwPChEREV1isQrYVX4aJ6prkZ4cj745qdColf8FkAEKERFRhFpfUoF5a0pRYap13Jepj8fc0bkozMsMYcu84zgIiaJSqfDJJ5+EuhlERCTS+pIKTF2x1yk4AQCjqRZTV+zF+pKKELVMHAYoClRcXAyNRoNRo0ZJel7Hjh2xcOHCwDSKiIjChsUqYN6aUgguHrPfN29NKSxWV1soAwMUNyxWAcVlVVi97xiKy6qC+iYuWbIE999/P7Zt24bjx48H7bhERBQZdpWfbtFz0pQAoMJUi13lp4PXKIkYoLiwvqQC1z+7GRPe3Inpq/Zhwps7cf2zm4PSHXbu3Dm8++67mDp1KkaNGoVly5Y5Pb5mzRr06dMH8fHxSEtLwy233AIAGDhwIA4fPowZM2ZApVI5ZsA8/vjj6Nmzp9M+Fi5ciI4dOzp+3r17N4YOHYq0tDTo9XrceOON2Lt3byBfJhERBdCJavfBiS/bhQIDlGZCPWb33nvvoUuXLrjqqqswceJEvPXWWxAEW+/NunXrcMstt2DkyJH47rvvsGnTJvTt2xcA8NFHH6Fdu3Z44oknUFFRgYoK8e2srq7GpEmTsH37duzcuROdO3fGyJEjUV1dHZDXSEREgZWeHC/rdqHAWTxNeBuzU8E2Zjc01xCwKVpLlizBxIkTAQCFhYUwmUzYunUrBg4ciKeeegrjx4/HvHnzHNv36NEDAJCamgqNRoPk5GQYDAZJxxw8eLDTz2+88QZSUlKwdetW3HTTTX6+IiIiCra+OanI1MfDaKp1eU1TATDobVOOlYo9KE2Eesxu//792LVrFyZMmAAAiImJwZ/+9CcsWbIEALBv3z4MGTJE9uNWVlZiypQp6Ny5M/R6PXQ6Hc6dO4cjR47IfiwiIgo8jVqFuaNzAdiCkabsP88dnavoeijsQWki1GN2S5YsQWNjI7Kyshz3CYIArVaLV155BQkJCZL3qVarHUNEdg0NDU4/T5o0CVVVVXjxxRfRoUMHaLVa5Ofno76+3rcXQkREIVeYl4lFE3u1qINiCJM6KAxQmgjlmF1jYyP+9a9/4R//+AeGDRvm9NjNN9+Md955B927d8emTZtw9913u9xHXFwcLBaL031t27aF0WiEIAiOxNl9+/Y5bbNjxw689tprGDlyJADg6NGjOHXqlEyvjIiIQqUwLxNDcw2sJBvuQjlmt3btWpw5cwaTJ0+GXq93emzcuHFYsmQJ/v73v2PIkCHo1KkTxo8fj8bGRnz22WeYNWsWAFsdlG3btmH8+PHQarVIS0vDwIEDcfLkSTz33HO49dZbsX79enz++efQ6XSO/Xfu3Bn//ve/ce2118JsNuORRx7xqbeGiIiUR6NWIb9Tm1A3QzLmoDQRyjG7JUuWoKCgoEVwAtgClG+//Rapqal4//338emnn6Jnz54YPHgwdu3a5djuiSeewKFDh9CpUye0bdsWANC1a1e89tprePXVV9GjRw/s2rULDz/8cItjnzlzBr169cIdd9yBBx54AOnp6bK/RiIiIrFUQvMEhTBgNpuh1+thMpmcegIAoLa2FuXl5cjJyUF8vG9DMeG8dkEkkeO9JCIi5fB0/W6OQzwuhPOYHRERUSRggOJGuI7ZERERRQJJOSiLFi1C9+7dodPpoNPpkJ+fj88//9zx+MCBAx1l1u23e++912kfR44cwahRo5CYmIj09HQ88sgjaGxslOfVEBERUUSQ1IPSrl07PPPMM+jcuTMEQcDy5csxZswYfPfdd7j66qsBAFOmTMETTzzheE5iYqLj/xaLBaNGjYLBYMDXX3+NiooK3HnnnYiNjcXTTz8t00siIiKicCcpQBk9erTTz0899RQWLVqEnTt3OgKUxMREt6XW//Of/6C0tBQbN25ERkYGevbsifnz52PWrFl4/PHHERcX5+PLICIiokji8zRji8WCVatWoaamBvn5+Y773377baSlpSEvLw+zZ8/G+fPnHY8VFxejW7duyMjIcNw3fPhwmM1m/PTTT26PVVdXB7PZ7HQjIiKiyCU5SfbHH39Efn4+amtr0apVK3z88cfIzbXVDrntttvQoUMHZGVl4YcffsCsWbOwf/9+fPTRRwAAo9HoFJwAcPxsNBrdHnPBggVOC+QRERFRZJMcoFx11VXYt28fTCYTPvjgA0yaNAlbt25Fbm4u7rnnHsd23bp1Q2ZmJoYMGYKysjJ06tTJ50bOnj0bM2fOdPxsNpuRnZ3t8/6IiIhI2SQP8cTFxeGKK65A7969sWDBAvTo0QMvvviiy2379esHADh48CAAwGAwoLKy0mkb+8/u8lYAQKvVOmYO2W/ku7vuugs333yz4+eBAwfiwQcfDHo7tmzZApVKhbNnzwb92EREpGx+l7q3Wq2oq6tz+Zh9UbrMTFv11fz8fPz44484ceKEY5sNGzZAp9M5homi2V133eWYnm0PBJ944omAT8P+6KOPMH/+fFHbMqggIqJgkDTEM3v2bIwYMQLt27dHdXU1Vq5ciS1btuCLL75AWVkZVq5ciZEjR6JNmzb44YcfMGPGDNxwww3o3r07AGDYsGHIzc3FHXfcgeeeew5GoxGPPfYYioqKoNVqA/ICfWaxAF99BVRUAJmZwO9+B2g0AT9sYWEhli5dirq6Onz22WcoKipCbGwsZs+e7bRdfX29bLOeUlPlX/yQiIjIH5J6UE6cOIE777wTV111FYYMGYLdu3fjiy++wNChQxEXF4eNGzdi2LBh6NKlCx566CGMGzcOa9ascTxfo9Fg7dq10Gg0yM/Px8SJE3HnnXc61U1RhI8+Ajp2BAYNAm67zfZvx462+wNMq9XCYDCgQ4cOmDp1KgoKCvDpp586hmWeeuopZGVl4aqrrgIAHD16FH/84x+RkpKC1NRUjBkzBocOHXLsz2KxYObMmUhJSUGbNm3wl7/8Bc2XX2o+xFNXV4dZs2YhOzsbWq0WV1xxBZYsWYJDhw5h0KBBAIDWrVtDpVLhrrvuAmDrSVuwYAFycnKQkJCAHj164IMPPnA6zmeffYYrr7wSCQkJGDRokFM7iYiImpLUg7JkyRK3j2VnZ2Pr1q1e99GhQwd89tlnUg4bXB99BNx6K9B8DcVjx2z3f/ABMHZs0JqTkJCAqqoqAMCmTZug0+mwYcMGAEBDQwOGDx+O/Px8fPXVV4iJicGTTz6JwsJC/PDDD4iLi8M//vEPLFu2DG+99Ra6du2Kf/zjH/j4448xePBgt8e88847UVxcjJdeegk9evRAeXk5Tp06hezsbHz44YcYN24c9u/fD51Oh4SEBAC2mVYrVqzA4sWL0blzZ2zbtg0TJ05E27ZtceONN+Lo0aMYO3YsioqKcM899+Dbb7/FQw89FPgTSERE4UkIQyaTSQAgmEymFo9duHBBKC0tFS5cuCB9x42NgtCunSDYwpOWN5VKELKzbdsFwKRJk4QxY8YIgiAIVqtV2LBhg6DVaoWHH35YmDRpkpCRkSHU1dU5tv/3v/8tXHXVVYLVanXcV1dXJyQkJAhffPGFIAiCkJmZKTz33HOOxxsaGoR27do5jiMIgnDjjTcK06dPFwRBEPbv3y8AEDZs2OCyjV9++aUAQDhz5ozjvtraWiExMVH4+uuvnbadPHmyMGHCBEEQBGH27NlCbm6u0+OzZs1qsa+m/HoviYhIcTxdv5vjYoFNffUV8Ntv7h8XBODoUdt2AwcGpAlr165Fq1at0NDQAKvVittuuw2PP/44ioqK0K1bN6e8k++//x4HDx5EcnKy0z5qa2tRVlYGk8mEiooKx2wqAIiJicG1117bYpjHbt++fdBoNLjxxhtFt/ngwYM4f/48hg4d6nR/fX09rrnmGgDAzz//7NQOAE4F/oiIiJpigNJURYW82/lg0KBBWLRoEeLi4pCVlYWYmEtvUVJSktO2586dQ+/evfH222+32E/btm19Or59yEaKc+fOAQDWrVuHyy67zOkxxSU/ExFRWGCA0tTF6dCybeeDpKQkXHHFFaK27dWrF959912kp6e7rQ2TmZmJb775BjfccAMAoLGxEXv27EGvXr1cbt+tWzdYrVZs3boVBQUFLR639+BYLBbHfbm5udBqtThy5IjbnpeuXbvi008/dbpv586d3l8kERFFJb/roESU3/0OaNcOUKlcP65SAdnZtu0U4Pbbb0daWhrGjBmDr776CuXl5diyZQseeOAB/HZxqGr69Ol45pln8Mknn+CXX37Bfffd57GGSceOHTFp0iT8+c9/xieffOLY53vvvQfAluSsUqmwdu1anDx5EufOnUNycjIefvhhzJgxA8uXL0dZWRn27t2Ll19+GcuXLwcA3HvvvThw4AAeeeQR7N+/HytXrsSyZcsCfYqIiChMMUBpSqMB7FVxmwcp9p8XLgxKPRQxEhMTsW3bNrRv3x5jx45F165dMXnyZNTW1jp6VB566CHccccdmDRpEvLz85GcnIxbbrnF434XLVqEW2+9Fffddx+6dOmCKVOmoKamBgBw2WWXYd68eXj00UeRkZGBadOmAQDmz5+POXPmYMGCBejatSsKCwuxbt065OTkAADat2+PDz/8EJ988gl69OiBxYsX4+mnnw7g2SEionCmEtxlSyqY2WyGXq+HyWRqMbRRW1uL8vJy5OTkID4+3rcDfPQRMH26c8JsdrYtOAniFONoJ8t7SUREiuHp+t0cc1BcGTsWGDMmJJVkiYiIiAGKexpNwKYSExERkWfMQSEiIiLFYYBCREREisMAhYiIiBQnYgOUMJycRM3wPSQiil4RF6DExsYCAM6fPx/ilpC/6uvrAQAazp4iIoo6ETeLR6PRICUlBSdOnABgK2amclcZlhTLarXi5MmTSExMdFqPiIiIokNEfvIbDAYAcAQpFJ7UajXat2/PAJOIKApFZICiUqmQmZmJ9PR0NDQ0hLo55KO4uDio1RE3CklERCJEZIBip9FomL9AREQUhvj1lIiIiBSHAQoREREpDgMUIiIiUhwGKERERKQ4DFCIiIhIcSJ6Fg8REVG4s1gF7Co/jRPVtUhPjkffnFRo1JFfH4oBChERkUKtL6nAvDWlqDDVOu7L1Mdj7uhcFOZlhrBlgcchHiIiIgVaX1KBqSv2OgUnAGA01WLqir1YX1IRopYFBwMUIiIihbFYBcxbUwpXa7rb75u3phQWa+Su+s4AhYiISGF2lZ9u0XPSlACgwlSLXeWng9eoIGOAQkREpDAnqt0HJ75sF44YoBARESlMenK8rNuFIwYoRERECtM3JxWZ+ni4m0ysgm02T9+c1GA2K6gYoBARESmMRq3C3NG5ANAiSLH/PHd0bkTXQ2GAQkREpECFeZlYNLEXDHrnYRyDPh6LJvaK+DooLNRGRESkUIV5mRiaa2AlWSIiIlIWjVqF/E5tQt2MoJM0xLNo0SJ0794dOp0OOp0O+fn5+Pzzzx2P19bWoqioCG3atEGrVq0wbtw4VFZWOu3jyJEjGDVqFBITE5Geno5HHnkEjY2N8rwaIiIiigiSApR27drhmWeewZ49e/Dtt99i8ODBGDNmDH766ScAwIwZM7BmzRq8//772Lp1K44fP46xY8c6nm+xWDBq1CjU19fj66+/xvLly7Fs2TL87W9/k/dVERERUVhTCYLgV53c1NRU/P3vf8ett96Ktm3bYuXKlbj11lsBAL/88gu6du2K4uJi9O/fH59//jluuukmHD9+HBkZGQCAxYsXY9asWTh58iTi4uJEHdNsNkOv18NkMkGn0/nTfCIiIgoSKddvn2fxWCwWrFq1CjU1NcjPz8eePXvQ0NCAgoICxzZdunRB+/btUVxcDAAoLi5Gt27dHMEJAAwfPhxms9nRC+NKXV0dzGaz042IiIgil+QA5ccff0SrVq2g1Wpx77334uOPP0Zubi6MRiPi4uKQkpLitH1GRgaMRiMAwGg0OgUn9sftj7mzYMEC6PV6xy07O1tqs4mIiCiMSA5QrrrqKuzbtw/ffPMNpk6dikmTJqG0tDQQbXOYPXs2TCaT43b06NGAHo+IiIhCS/I047i4OFxxxRUAgN69e2P37t148cUX8ac//Qn19fU4e/asUy9KZWUlDAYDAMBgMGDXrl1O+7PP8rFv44pWq4VWq5XaVCIiIgpTfleStVqtqKurQ+/evREbG4tNmzY5Htu/fz+OHDmC/Px8AEB+fj5+/PFHnDhxwrHNhg0boNPpkJub629TiIiIKEJI6kGZPXs2RowYgfbt26O6uhorV67Eli1b8MUXX0Cv12Py5MmYOXMmUlNTodPpcP/99yM/Px/9+/cHAAwbNgy5ubm444478Nxzz8FoNOKxxx5DUVERe0iIiIjIQVKAcuLECdx5552oqKiAXq9H9+7d8cUXX2Do0KEAgBdeeAFqtRrjxo1DXV0dhg8fjtdee83xfI1Gg7Vr12Lq1KnIz89HUlISJk2ahCeeeELeV0VERERhze86KKHAOihEREThJyh1UIiIiIgChQEKERERKQ4DFCIiIlIcBihERESkOAxQiIiISHEYoBAREZHiMEAhIiIixWGAQkRERIrDAIWIiIgUhwEKERERKQ4DFCIiIlIcBihERESkOAxQiIiISHEYoBAREZHiMEAhIiIixWGAQkRERIrDAIWIiIgUhwEKERERKQ4DFCIiIlIcBihERESkOAxQiIiISHEYoBAREZHiMEAhIiIixWGAQkRERIrDAIWIiIgUhwEKERERKQ4DFCIiIlIcBihERESkOAxQiIiISHEYoBAREZHiMEAhIiIixYkJdQOIiIh8YbEK2FV+Gieqa5GeHI++OanQqFWhbhbJhAEKERGFnfUlFZi3phQVplrHfZn6eMwdnYvCvMwQtozkwiEeIiIKK+tLKjB1xV6n4AQAjKZaTF2xF+tLKkLUMpKTpABlwYIF6NOnD5KTk5Geno6bb74Z+/fvd9pm4MCBUKlUTrd7773XaZsjR45g1KhRSExMRHp6Oh555BE0Njb6/2qIiCiiWawC5q0pheDiMft989aUwmJ1tQWFE0kBytatW1FUVISdO3diw4YNaGhowLBhw1BTU+O03ZQpU1BRUeG4Pffcc47HLBYLRo0ahfr6enz99ddYvnw5li1bhr/97W/yvCIiIopYu8pPt+g5aUoAUGGqxa7y08FrFAWEpByU9evXO/28bNkypKenY8+ePbjhhhsc9ycmJsJgMLjcx3/+8x+UlpZi48aNyMjIQM+ePTF//nzMmjULjz/+OOLi4nx4GUREFA1OVLsPTnzZjpTLrxwUk8kEAEhNTXW6/+2330ZaWhry8vIwe/ZsnD9/3vFYcXExunXrhoyMDMd9w4cPh9lsxk8//eTyOHV1dTCbzU43IiKKPunJ8bJuR8rl8yweq9WKBx98EAMGDEBeXp7j/ttuuw0dOnRAVlYWfvjhB8yaNQv79+/HRx99BAAwGo1OwQkAx89Go9HlsRYsWIB58+b52lQiIooQfXNSkamPh9FU6zIPRQXAoLdNOabw5nOAUlRUhJKSEmzfvt3p/nvuucfx/27duiEzMxNDhgxBWVkZOnXq5NOxZs+ejZkzZzp+NpvNyM7O9q3hREQUtjRqFeaOzsXUFXuhApyCFHsFlLmjcxVZD4V1W6TxKUCZNm0a1q5di23btqFdu3Yet+3Xrx8A4ODBg+jUqRMMBgN27drltE1lZSUAuM1b0Wq10Gq1vjSViIgiTGFeJhZN7NWiDopBwXVQWLdFOkkBiiAIuP/++/Hxxx9jy5YtyMnJ8fqcffv2AQAyM21vQH5+Pp566imcOHEC6enpAIANGzZAp9MhNzdXYvOJiCgaFeZlYmiuISx6JOx1W5oPSdnrtiya2ItBiguSApSioiKsXLkSq1evRnJysiNnRK/XIyEhAWVlZVi5ciVGjhyJNm3a4IcffsCMGTNwww03oHv37gCAYcOGITc3F3fccQeee+45GI1GPPbYYygqKmIvCREReRRuwyTe6raoYKvbMjTXoJjXoZRzrBIEQXQ1G5XKdQOXLl2Ku+66C0ePHsXEiRNRUlKCmpoaZGdn45ZbbsFjjz0GnU7n2P7w4cOYOnUqtmzZgqSkJEyaNAnPPPMMYmLExUtmsxl6vR4mk8lpv0REFLnCcZikuKwKE97c6XW7d6b0R36nNkFokWeBPsdSrt+SAhSlYIBCRBRd3A2T2L82K3WYZPW+Y5i+ap/X7V4c3xNjel4W+AZ5EIxzLOX6zbV4iIhI0cK5vH241G1R4jlmgEJERIoWzuXt7XVb3GVwqGAbQgl13RYlnmMGKEREpGjhXN7eXrcFQIsgRUl1W5R4jhmgEBGRooXLMIk79rotBr1z+wz6eMXkzijxHPtcSZaIiCgYIqG8vdLrtijxHLMHhYiIFC1chkm80ahVyO/UBmN6Xob8Tm0U1V4lnmMGKEREpHjhMEwS7pR2jlkHhYiIwoZSqpxGskCeYynXb+agEBFR2LAPk1DgKOUcc4iHiIiIFIcBChERESkOAxQiIiJSHOagEBERNcNk3NBjgEJERNTE+pIKzFtT6rQ2TaY+HnNH53I6cxBxiIeIiOii9SUVmLpib4uF84ymWkxdsRfrSypC1LLowwCFiIgItmGdeWtKXZZ6t983b00pLNawKx8WlhigEBFRxLBYBRSXVWH1vmMoLquSFEzsKj/douekKQFAhakWu8pPy9BS8oY5KEREFBH8zR05Ue0+OPFlO/IPe1CIiCjsyZE7kp4c73UbKduRfxigEBFRWJMrd6RvTioy9fEtVvO1U8HWI9M3J9WP1pJYDFCIiCisyZU7olGrMHd0LgC0CFLsP88dnct6KEHCAIWIiMKanLkjhXmZWDSxFwx652Ecgz4eiyb2Yh2UIGKSLBERhTW5c0cK8zIxNNfASrIhxgCFiIjCmj13xGiqdZmHooKtB0RK7ohGrUJ+pzZ+t40l833HAIWIiMKaPXdk6oq9UAFOQUrT3BEAKC6rClqwwJL5/lEJghB2JfHMZjP0ej1MJhN0Ol2om0NEFFHC9Vu/p4AAQFCDBfu05+YXWPtZjNZ8FinXbwYoRETkEO7f+l0FVxtKjUENFixWAdc/u9ntzCL7kNP2WYPDIvCTk5TrN2fxEBERgMhYKM+eOzKm52WOHJJgr6/DkvnyYIBCREQRu1BeKIIFlsyXBwMUIiKK2G/9oQgWWDJfHgxQiIgoYr/1hyJYYMl8eTBAISKiiP3WH4pggSXz5cEAhYiIIvZbf6iCBZbM9x+nGRMREYBLs3gA18XOwvnCGqrp0+FaUyZQAjbNeMGCBejTpw+Sk5ORnp6Om2++Gfv373fapra2FkVFRWjTpg1atWqFcePGobKy0mmbI0eOYNSoUUhMTER6ejoeeeQRNDY2SmkKERHJLJK/9RfmZWL7rMF4Z0p/vDi+J96Z0h/bZw0O+GtqPu05moMTqSSVut+6dSuKiorQp08fNDY24q9//SuGDRuG0tJSJCUlAQBmzJiBdevW4f3334der8e0adMwduxY7NixAwBgsVgwatQoGAwGfP3116ioqMCdd96J2NhYPP300/K/QiIiEk2uhfKU2HMg1/o6FBx+DfGcPHkS6enp2Lp1K2644QaYTCa0bdsWK1euxK233goA+OWXX9C1a1cUFxejf//++Pzzz3HTTTfh+PHjyMjIAAAsXrwYs2bNwsmTJxEXF+f1uBziISJSrnCvRkuBE7RKsiaTCQCQmmpLmtqzZw8aGhpQUFDg2KZLly5o3749iouLAQDFxcXo1q2bIzgBgOHDh8NsNuOnn37ypzlERBRikVCNlpTB5wDFarXiwQcfxIABA5CXlwcAMBqNiIuLQ0pKitO2GRkZMBqNjm2aBif2x+2PuVJXVwez2ex0IyIiZYnUarQUGj4HKEVFRSgpKcGqVavkbI9LCxYsgF6vd9yys7MDfkwiIpImUqvRUmj4FKBMmzYNa9euxZdffol27do57jcYDKivr8fZs2edtq+srITBYHBs03xWj/1n+zbNzZ49GyaTyXE7evSoL80mIooIFquA4rIqrN53DMVlVUHtkfB07EitRkuhIWkWjyAIuP/++/Hxxx9jy5YtyMnJcXq8d+/eiI2NxaZNmzBu3DgAwP79+3HkyBHk5+cDAPLz8/HUU0/hxIkTSE9PBwBs2LABOp0Oubm5Lo+r1Wqh1WolvzgiokgTygRUb8cOdDVaJc4MosCRNIvnvvvuw8qVK7F69WpcddVVjvv1ej0SEhIAAFOnTsVnn32GZcuWQafT4f777wcAfP311wBs04x79uyJrKwsPPfcczAajbjjjjvwP//zP6KnGXMWDxFFI3sCavMP7WAUUhNz7KG5Blz/7GYYTbUu81BUsNVU2T5rsOTAIlCBGYOe4JJy/ZYUoKhUrt+0pUuX4q677gJgK9T20EMP4Z133kFdXR2GDx+O1157zWn45vDhw5g6dSq2bNmCpKQkTJo0Cc888wxiYsR16DBAIaJoY7EKuP7ZzW5zPPy5+Mt57A2lRtmr0QYqMON06OALWICiFAxQiCjaFJdVYcKbO71u986U/rIXI5N6bDkv/IEKzELZGxXNpFy/JeWgEBFRaIQyAVXqseWqRgtImxkkNjDzNh1aBdt06KG5Bg73hBADFCKiMBDoBFS5jy1XWflABGaBCHpIfn5VkiUiouDom5OKTH083H2fV8E2jNI3JzWijh2IwIzTocMDAxQiojCgUaswd7StFEPzQMH+89zRuQEZkgjlsQMRHIWyN4rEY4BCRBQmCvMysWhiLxj0zhdOgz4+4EmdoTp2IIKjUPYIkXicxUNEFGZCWbsjEMcWs0+5pwTbZ/EA8k2HJu84zZiIiMKClMBD7uCIdVCCjwEKEREpnhJqkbCSbHCxDgoRESmaUmqRyDUdmuTHJFkiIgo6KbVIKDqxB4WIKExE0nAEa5G0FEnvrxwYoBARhYFIS+hkLRJnkfb+yoFDPERECmdPJm0+JGI01WLqir1YX1IRopb5jrVILonE91cODFCIiBTMWzIpYEsmtVjDa0JmKKvTemOxCiguq8LqfcdQXFYV0HMbqe+vHDjEQ0SkYJG8sJ29Om3zoQ1DCIc2gj3UEsnvr78YoBARKVikJ5MW5mViaK5BEcmh7uqy2IdaAlGXJdLfX38wQCEiUrBoSCZVQi0SKXVZAMgWUEXD++srBihERApmTyY1mmpdXjxVsA2JREMyaSCJHWp5ZfNBrNp9RLYhIL6/7jFJlohIwZScTBpJxA6hvLDxv7LOtuH76x4DFCIihbMnkxr0zt38Bn08V92ViT9DKP7OtuH76xqHeIiIwkCgkklZvdTG21CLN/7OtlFSsrBSMEAhIgoTcieT+jqlNhKDGvtQy9QVe6ECnIKU5j974s9sGyUkCysJAxQioijk65TaSC7J7qkuy/g+2Xhh4wGv+4jG2TaBohIEIezK05nNZuj1ephMJuh0ulA3h4gorFisAq5/drPbWSv2mSPbZw126hlxF9TYt4iUfAlXPUQAcP2zm73Otml+zsiZlOs3k2SJiKKMlOqldtFUkt0+1DKm52XI79QGGrWKs21CgAEKEVGU8aV6qS9BjRyCuS6ON5xtE1zMQSEiijK+VC8NRUl2V/kuqUmxeHJMHkZ2z5LtOFJwtk3wMEAhIooyvlQvDXZJdnf5LqdrGnDfyu/wv7+dxeyRubIcSyrOtgkODvEQEUUZX/Ip7EGNu34CFWyzeeQoye4p38Xu9W3l+OwH6ZVbKXwwQCEiikJS8yk8BTWALQdlzih5kkS95bvYzVldEhFJueQah3iIiKKU1HwKd3VC7OavK4VaDb+TRcXmsVTV1PtcuVWJIrEAnj8YoBARySQcLzBS8ykK8zJhtQq4b+V3LR7zVuRNLCl5LHIm5YZSJBfA8xUDFCIiGUTLBcZiFTB/3c8uH2taD2VorsHn4KxvTipSk2JxuqbB67aRULnV16q+kY45KEREfrJfYJoPe9gvMOtLIieZU0x+iKsib1JqmWjUKjw5Js9rW/xNylVCjZVoKoAnFXtQiIj84O0Co4LrHoVwHA4CxA+pbCg1Ir9TG597lkZ2z8L//nYWr28rd/m4Cv5VblVKj5eUAniRkmsjluQelG3btmH06NHIysqCSqXCJ5984vT4XXfdBZVK5XQrLCx02ub06dO4/fbbodPpkJKSgsmTJ+PcuXN+vRAiolDwpcLq+pIKXP/sZkx4cyemr9qHCW/uxPXPbg6LnhaxQyqr9x3HZz8c96tnafbIXLx2Wy+kJsU53Z/pZ+VWJfV4haIAXriQ3INSU1ODHj164M9//jPGjh3rcpvCwkIsXbrU8bNWq3V6/Pbbb0dFRQU2bNiAhoYG3H333bjnnnuwcuVKqc0hIgopqReYcM03sPf4GM21SI7XoLrW4nH7qpp6zProB8k9S82N7J6J4XnyVW71tccrUIJdAC+cSA5QRowYgREjRnjcRqvVwmAwuHzs559/xvr167F7925ce+21AICXX34ZI0eOxPPPP4+srNCULyYi8oWUC4zSLo5iuRoOEcNTECNl6ELOyq1KG1LxpapvtAhIkuyWLVuQnp6Oq666ClOnTkVVVZXjseLiYqSkpDiCEwAoKCiAWq3GN99843J/dXV1MJvNTjciIiWQUmE1VAvu+cPdcIhcgj10obQhFa6S7J7sAUphYSH+9a9/YdOmTXj22WexdetWjBgxAhaLLZI2Go1IT093ek5MTAxSU1NhNBpd7nPBggXQ6/WOW3Z2ttzNJiKSzD7sMTLP4PbbL3DpAiP2omc0XZCtjf4QU3LeX8EeulDikApXSXZN9lk848ePd/y/W7du6N69Ozp16oQtW7ZgyJAhPu1z9uzZmDlzpuNns9nMIIWIAs7TTBtXwx5qFdB0Nqih2awQsRe9+et+RkKcJuQXJrEl530RqqELpQ6pcJXklgI+zfjyyy9HWloaDh48iCFDhsBgMODEiRNO2zQ2NuL06dNu81a0Wm2LRFsiokDyNA0VgMtEV+HiHX8e0BFDcw0tLjDeLo52Z2rqFZEwG+hhjlAMXdiHVKau2AsV4PQ+yDWk4usUcq6S7CzgAcpvv/2GqqoqZGba/sjy8/Nx9uxZ7NmzB7179wYAbN68GVarFf369Qt0c4iIvPI00+beFXuRkhjrMdH1sx8rMKRrBtb+cNzpAtX04uiJUhJmxfb4zBnVFWnJWpyqrnNbZbap1KRYPH1Lt5AFX+7WFGre4+ULpdRXiQQqQRAkDS+eO3cOBw8eBABcc801+Oc//4lBgwYhNTUVqampmDdvHsaNGweDwYCysjL85S9/QXV1NX788UdHL8iIESNQWVmJxYsXO6YZX3vttaKnGZvNZuj1ephMJuh0OokvmYjIPYtVwPXPbpZ1aKP5BWp9SQX++vGPokq5vzOlv+NbdbCLu1msAgY8sxlGs+tzYR8O2T5rMDRqlePceeohapMUh+LZQxAXE/pC5nKfT3eBrX2Poe4RUwIp12/JPSjffvstBg0a5PjZnhsyadIkLFq0CD/88AOWL1+Os2fPIisrC8OGDcP8+fOdhmjefvttTJs2DUOGDIFarca4cePw0ksvSW0KEYU5JVZTDUTeRfMaJ4V5mbjQYMWMd/d5fW7T+iliv5nLdV43lBpR2+h6qrCr4RAxwydP3ZKniOAEkHdIJVynkCuZ5ABl4MCB8NTp8sUXX3jdR2pqKouyEUU5pXaFByLvwtUFyqATP5tESnE3uc6ru2PapSTGYsHYlsM0gRw+UTKl1VeJBFyLh4iCxv7NfkOpEW/tONTicSVUUw3U9NLmFyixs0l6d2iNG//+pahv5htKjbJUqRUzvVgbo8bQXNcTG6JxRorS6qtEAmX0sxFRxGu6/oyr4ARQxuqtYgqvpSTG+rx/+wWqaYEuVwQAv++RiT2Hz4j6Zv7W9nI8+uGPsqyKK2aYy2iu81hQzj58MqbnZcjv1CaigxNAmfVVwh0DFCIKOCnVSJVQTXV8n2y3vRr+hk1NL1CFeZm454Yct9u+sa0cG0pdF7Bs7qnPfsbZC+6TbqWcVzl6AyxWAcVlVVi97xiKy6pCFnAGi5SKwiQOh3iIKKB8rUYaiq5wb2vOpCTG4sz5Bpw97zoQUKku1UJp8RhaFgCzWAV8+r3nlXNX7zsuqu1iiTmvUtcXaj6Us6HUqMj8oubkTNIORn2VaMMAhYgCytdZMcHuCveWFPrgkM5YtfuIx314Ck6AlhcoMYmVVTX1SE2KxZmaBllKzos5r2LzY87U1LeYkp2SGOsygFNCflFTgUjSjtYE4UBhgEJEASW1JyQUpca99fKoAPx752FU1dSL2p+rkvdzRuVCnxCH1fuOOb6tiz03t/S8DG/tOOTXEJOY89q0R2F8n2y8sPGA296A3/fIRNHKlgGdu94le0Lv45/+hOT4WJw6Vxey5Fkps6K8ad4LMzTXEHUJwoHCAIWIAkpKT0iousLF9mSI1Tzd4nx9I/76yY9OF+9MfTzG9xG3plhBrgF9clI9Dj95I8DzeXXVo2BPBm7abluw1RXz1/0sOVgSYEuuvf3/XVq53qDT4vHfXx203gU565Uodap8pGCAQkQBJXb9GSB0XeGBzncxXWhscZ/RVIsXNh5ASmIsTOddD9807fXQqFVO38xPmOvw1Gfey8qL4a5H4ez5BqgAzCi4Eh3TEh29AXIWszOa63Dvir1Y7KbXQu5ifnLVK5GzF4Zc4yweIgqoptNp3V1WJg/oiHem9Mf2WYND8qEutpcnNcn36cXN2b+t2zU/N+4qteZ3agNtjBpLtv8q6Xj2noHms2m8DW8JAFbtPoK0JC1OVNsu3O5K3/vj0Y9+bNG2plPTp6/ahwlv7sT1z27G+hLPicWeiA1GN3qYPeWtFwYI7VT5SMEAhYgCzp48aNA7BwKZ+ngsntgLc0ZfHZRaGe6mvoqdIvrkmDxZ2yPA1kvxYMGVLc6NQR/v8lu4/Zu70Vwn+ViuphmL6Q2pMNXi9iXfOIKEJ9aUSDq2GGfPN2BnWZXjZ3dT0+09FL4GKWKD0SU7Drk9hpReGPIdh3iIKChCXV3UW76AmCmihXmZWKxW4dGPfnSbDOqLjmmJ2D5rsNdz4+uU7aaa9yAYTRck7+PM+ZZDVnIo/vUUBnROC+i6NlKGHN0dg1Vjg4M9KEQUNKGqLirm27i7Xp7mPRmFeZnY89hQzCi4EikJ8gz5pCfHizo3cuR+NO9BOHVOfPJv4NlecyB7KOxDjmKCPHfHYNXY4GAPChFFNCnfxsX28mjUKkwv6Ixpg69wbJvWSouH3tuHSnOd6B4OqVOqfentcHcsi1XAK5sPYPHWMp/3KTd7UmqgeygK8zLx5wEd3S654O0YYuvEsGqsf9iDQkQRTeq3cSm9PE23HXBFGh7//dUA3CcDN+XLlOrTEqY6ezrW+pIK9H5yA17YeAAXGqw+7VNurRNj0f9yW4ASjB4KdwsdijmGp8RvVo2VDwMUIopowcwXcDdMlJIY22KBQXdJsJ6kttL61K6mx7IPd8mZQyOHBWO7OS7owVjX5kxNHTzFD96OIXZIkHzHIR4iimjBzhdwN0wEwO8EYYPOtzYKF2vwy5FkKzdXhc0Cva7N+pIKFK38zut5aHoMV/VYQp34HekYoBBRRAtFvoB96Kc5T4W/xLC/FqmJspXmOkxdsRcPFnSW9NykOA1q6i1Smylam6Q4bH1kEOJiWnbmB2pdGzFBmloFvDLhUi+Itxlg/r6v5BoDFCKKaOGyyqyYiqlNXwsgfl0eezLwUhFJoU29cce12HbgBF7fVi7peWJV1dTj38WHkJasdfmaA9FDIWYmlFUAWifFAWDF2FBigEJEEU/pq8xKWdPF3WvxRgBw9oL4vJNMfTz65KSi6J29op/ji/nrLpXrdzfcI2cPhZScpEDWYyHvVILgboFw5TKbzdDr9TCZTNDpdKFuDhGFCV/WdZH6HKnbu/uGbn9G02/oTfed1koLCMCpmjqkJWnxddkpvLrF+5RhfUIsTCIClckDOsJ0oQEf7D3mdVu5uHrNcisuq8KEN3d63e6dKf0BQPS2HOYRR8r1mz0oRBQ1pH4bl7pardTtpXxD31BqdLvvAZ3ToFarRAUoFqvnacUqFSAItlLvwSZ3r4SrYFFMTlKGTgurIOCLn9yvx9MUK8YGBntQiCgk5F6lVm5SejZ82R4Q/21+RkFnLNx4wOW+BQB/HtARQ7pk4KH3v0el2XsJ90BpmuPTPN9HKn97JTwFiwBc5vHY25ySGCtpGjZ7UMRjDwoRKZrUnoZgk5p7IGb7//u4BBfqLTDoExzBmNhv3m9s+9Xjyrlv7TiEt3Ycgj4hxm2vQDCClpTEWCwY2w0AJOfINOdPr4SYxFZXeTz6i4GJ2OCEFWMDiwEKEQVVOMyKEFt9dtmOcqQla3Gqus7r9lU19Zjx3vcALgVjYmuviJ3qa7rgehG/JG0MztUFZoG/ps5cvLDbZ9/s/LUKRW/vlZSca3equg6r9x2T3LsmNrjcPmuw0wyhtCQtHnr/ewDigxNAGTPAIhUDFCIKmnCZFSH223vTGShS2IOxV2+7RvTKuv6wBmkkv/n7N+CKNDwzrpvkadFqlffZPe5IWdogv1Mbx9BMcVkVjGbxvTZKmQEWyVjqnoiCJpCr1Mop0KvQ2i/U89f9jP8b0TXgwy/nA1hsramm75/FKqC4rAp1jVY8WNAZGTrxZfqtzU5I01WnvfF1aQOxz0uM1WBGQWdsnzWYwUmAsQeFiIImmOvi+KNvTqrkREmp7Bfzv635KWDHCJUNpUbMfG+fc80ZXTxmFFyJjmmJSE+Ox5maesxf55wDola1DE4Aab1rvi5tIPZ55xssWLjxAK4yJDNACTAGKEQUNMFeF8dXG0qNQVtMz9cVioPFHgo8WHAlGixWvPLlQa/PecvFFOVKcy0WbvwvFk3s5RhWGZ53KQfkVHWdxyGz5kMz7vi6tIG35zWnhKHISMchHiIKmmCsUusve55MtHK36vL0gs6YMfRKj+8fALcrBNsv+vPWlMJysZvEXpdmTM/LkJYsbgjIW++afTkAAC3a6SmxtenzvFHKUGSkY4BCRG7Z8whW7zuG4rIqx4XFV75ePIJJzFot3swZ1RUv/KknUpNivW8skS4+Bndf10HSc1pfDDo8ndVMfTwWT+yFPY8NxTtT+uPF8T3xzpT+TrkWYt4/T78ini7scvau2ZcDMOidt7UHW+6GZuzPS0kQ976Feigy0nGIh4hcClStEqWvi+PPRcc+fHDXgBxo1Cr8UmGSfaG9W3u3Q9+cVCz9+rDo5wgA/veGHHz6fYXTOdfFx+Ca9q1xQ+c03JHfERq1ymvxPE/v38g8g6gKtK7OsdyrTvu60GBhXiaStbG4fck3Xo8R6qHISMcAhYhaCHStkkCsUisXXy86zXuALFYBn37vfdaJVEO6ZODhD76X9BzT+Qa8sa0cr952DVonabGx1IiP9x3D6ZoGbP3vSWz970m8fDG3pGnuTaY+HnNG5aJ1UpzT++Tu/dtVflpUgOLqHDdfqbkpX3vXfF1osH+nNrIGS+QbBihE5CRYtUrkXqVWLmKSJVsnxkIbo4bRXOe4r3kP0M5fq/weKnLlo+9+k7xf+/s2f93PmDMqF2/tONTitblKCq4w1eK+lc4BQ9NetObvX9+cVBh08W7riYi5sOtdzJ6yV6j1FhTLtXxC02CpeRVepQxFRgMGKETkRGqhq0jj6Zu83R+vbYe/FHZ1ezFcX1KBRz/8MSDt+9DH1YXt79tjq0v8qrviqRdtQ6kRtY2ua654u7C767UDLlWo9UTuIUmlD0VGA8lJstu2bcPo0aORlZUFlUqFTz75xOlxQRDwt7/9DZmZmUhISEBBQQEOHDjgtM3p06dx++23Q6fTISUlBZMnT8a5c+f8eiFEJI9wqVUSSIV5mbjnhhy3j7+xrRwbSo2OGShNq5HOX/MT7l3hW3n3YPB3WrOr2TjApQDD3fTslMRYt0ODnnrtgEu9du6StO3Hbh5YSynw5kphXia2zxrsNmmYAktygFJTU4MePXrg1Vdfdfn4c889h5deegmLFy/GN998g6SkJAwfPhy1tZd+cW6//Xb89NNP2LBhA9auXYtt27bhnnvu8f1VEJFswqVWiRzczVLylj8iwPmCub6kAtc/uxkT3twpKgcj3DWfjeMtwAAAbYwaQ3MNLh/zp8KwtyFJwHNw403TqdD5ndpwWCeIJA/xjBgxAiNGjHD5mCAIWLhwIR577DGMGTMGAPCvf/0LGRkZ+OSTTzB+/Hj8/PPPWL9+PXbv3o1rr70WAPDyyy9j5MiReP7555GVleXHyyEif8k9m0KpPA0J6BPivOZ52C+Ypgv1bocmIt2OgycdybHezpfRXOdyWNBiFbDj4ElRx3PVaxftQ5KRTNY6KOXl5TAajSgoKHDcp9fr0a9fPxQXFwMAiouLkZKS4ghOAKCgoABqtRrffON6WlddXR3MZrPTjYgCIxxqlfjL3ZBAhakW967Yiy9EDglUnL3gtecgkr3yZRmuf3YzNpQaRW3fPMCw9zy98mWZqOe76rXzZ0hS7jo/JC9Zk2SNRtsvaUZGhtP9GRkZjseMRiPS09OdGxETg9TUVMc2zS1YsADz5s2Ts6lE5EG4JAiKmbXRfJveHVp7DSqWFYurMfLd0TMBmakTToymWpel7V1pGmB4SoptzlOvna9DkoGq80PyCYtZPLNnz8bMmTMdP5vNZmRnZ4ewRUSRT8m1SgBxFxhX26Qmxcm2/k3F2fOy7Cec2acwq9ws9GeXkhjrCDDE5KzYeeu182VIMtB1fkgesg7xGAy2BKjKykqn+ysrKx2PGQwGnDhxwunxxsZGnD592rFNc1qtFjqdzulGRIEXigRBMd3uYmZtuNtGzsX5Nv5ySrZ9hTMBnoMTwFZnxT4UJGU5AW/l6aUOSQY6qZbkI2sPSk5ODgwGAzZt2oSePXsCsPV2fPPNN5g6dSoAID8/H2fPnsWePXvQu3dvAMDmzZthtVrRr18/OZtDRGFGTK+ImEJyj3/6EwBVWOaGpLgoVBYukuI0qKl3XwfFXuBPbN7ItEFXYMbQK0WVpxc7JMmk2vAhOUA5d+4cDh68tNx2eXk59u3bh9TUVLRv3x4PPvggnnzySXTu3Bk5OTmYM2cOsrKycPPNNwMAunbtisLCQkyZMgWLFy9GQ0MDpk2bhvHjx3MGD1GEcJX3sefwGY9DRWK73cVcYJpWeA0Xd1/XAcOuzkTfnFRsKDVi7uoSVFZf6u3JSI7D3NFXo3WSFtsPnMSrW8QllgaTu+AEcL7wi80bGXBFmuheO7FDkqzzEz4kByjffvstBg0a5PjZnhsyadIkLFu2DH/5y19QU1ODe+65B2fPnsX111+P9evXIz7+0i/k22+/jWnTpmHIkCFQq9UYN24cXnrpJRleDhGFmqteEHWz/ARfekX+7+MSXKi3oOxkZBZ1XP39cTx209WOC6pK5TwCX28RsOfwGRTkGnBFeqtQNFEWJ6prcVP3rIBMZRezfEI01fkJdypBEMKuF9RsNkOv18NkMjEfhUhBxM7MsH+ntfeKFJdVYcKbOwPdPElUAPQJsUGtCDtnVFccP3vBa7G3VtoYnKtrDE6jZPbOlP7I79TG8bsCuF7rpnneiVzr7FisAq5/drPX4Gj7rMGKSQiPJFKu37ImyRJR9JIyM6N5MqLc3emtE/1Lr7Nflu4e0NHvtkgxf93PoirRhmNwooKt58zeK2LPGzHonXsqXCXFNq3UO33VPkx4cyeuf3azTyXso6HOT6QIi2nGRKR8UmZmAL7lJIjVq31rbPpFXHVSoOXUY3ty5dBcA1btPupxZWPyzt2FX0zeSCCmBIdLnZ9oxwCFiAD434Xuay+ImJwEqb47elb0tpn6eGx9ZJAjiTctSQuogFPnbKXZ54zKRdHKvVABDFJ8lJoUh6duyXN54feUNyImN8k+M0hqj4fS6/wQAxQigjxVNX3tBUlPjnd0u09dIU8gcLqmAalJcThTU+91X3NH5yIuRu3Ii3j4g+9bnId7bsjBp99XRH3VWF89NqqrT70SgZ4SLCaplkKHOShEUU6upertFT3Ffv8Um5Pgq57ZelHb7TdWY/W+Y3hx439xr5vz8Ma2cswZ1RW39Iy+UggqGToUDPoEn57HKcHRjQEKURSTs6qmp+TD5jzlJGyfNRhv/08/FA3qhPhY3z+ivj9qwsvje8Jbj/0LGw9g+qp9eGHjAZeP21/5/HU/Y2yvdj63J1wl+PEeNA9CpeKU4OjGAIUoiknpQhejMC8Tr952DVonxTrd3zxI8FS+/IuSCtz/zl68+mUZahusoo7rSlVNPSqr67yWYBfDfh7e2SVuEcFAs5/OxDhNwI91vt79e9C7Qwomu5np1DwI9WXlYDG9cqlJsejdobXXfVH4YQ4KURSTuwt9fUkF5q/7GadrLtUOSU2KwxOjr0abZK3XZMQFn5Xi9W3l4hovwjciAyuxPiup9L5REJ33ULk1GPYcPovJA3LQJyfV44wYX3OcxOQmna5pwI1//5KzbyIQC7URRTGxBdLsxbU8cTcd1F3hreY++6EC963c67Ut0apnth4/V1SjrtH3XqVASE2Kxc7ZBdh96DSKy6oACMi/PA39Ly4u6e/vBeA6idvXfVFoSbl+M0AhimJyVdW078fTBcTTfixWAX2e2ijrSsMUPM3ryGQ2qSPjz+9FU/WNVvRfsMnt7wgrwIYHVpIlIlHkqqrpby7LrvLTDE7CWPP3zj4D7JXNB2TLcdpz+IzH3xGp+VKkfAxQiCKIL4mInkqOv3rbNdAnxHndn9gclc9LKlzuh9NEI4v93V0qomw/IO7955Tj6MMkWaII4U+xNVdVNc/U1GP+OnH7EzvN81/Fh/Gv4sMw6OLx+O8v7YfTREPvgcFXQK1SYeEm19OtpRIA0Qstinn/OeU4+rAHhSgCyFFszV5Vc0zPy2C6UI+ileL3J7VIm9Fci3tX7MVnP1Q4PZ9CZ/nXh/DiZnmCk6ZSEmLd/l5IqZPi7XfM35orpDwMUIjCnJzF1nzdn5QibU0VvbMXn/1w3On5FBqm2kZImTLRSiuuBsvdA3IA+L9yMFchjj4MUIjCnK8Jqu7yVcTub9mOcqfn2oq09ULrpDjRbRcE4L6V34kup98cr0XBlxinwYyCK7F3zjBRPRrTBl/hNsdJ6rRgT/lSnGIceZiDQhTmfEke9JSvIrbOxvx1Pzs99/c9MvHp9xU+zcZ5/NOfIK3vxWby9Tn4f1/ZCruFXb2EMHXP73IwvaAzALgtota8R0POlYO5CnH0YIBCJJHFKijqwzGtlVbUduUnawC4L6hmzy95sOBKyW2oMNX6VQHWaK6T/BwVgLU/VODV23q1SOalwFm46SC6ZOpQmJfp6NHwVEXWTs6Vg+Xal9L+lskZAxQiCfyZKRMwIrsOFm46gCszWmH+up/d5peoAKzafQStE2Nw5nyjjI2Un32oqXVSHOaM6or7Vn4X6iZFBRVsOUhDcw2y944EkyL/lskJc1CIRJJjpkwgnKoR3/vw2OoSUfklSg9OmjKaa52GmyiwXOU0NZ0Bln+xxL2SKfVvmZwxQCESQe6ZMnKSUveh6SJ+keJUdR2Hd0JAroJovhQX9Pd4Sv1bJmcc4iESQcpMGbnG2cWy14eI1ov0t4dZ2tzdSr/6hBiYLgSmN0yOgmihGGZR8t8yOWMPCpEIgSqzLce3R6k1RFKT4nyYL6NcOw6eCnUTQiopTgN9YqzTfZn6eCye2Avj+2QH5JipSbF+F0QL1TALS+aHD/agEInga5ltT7ME5Pz2WJiXidduuwbT3vkO7mIc+2qvc0blomhly6mh4epcnQWxGhUaLJHwaqSrqbdAVW/BjIIr0TEt0fF7BtiGKgLhlp6X+ZVn4m2YpXkirpxYMj98MEAhEsE+jGI01br8ULVf/Jt+q/QUgADwONVXStEpexDUYBXwwODOLtdSaVqXojAvE4vULaeGhrNoDU7sBABvfvUr9s4ZirgYW8d4cVlVwN7fglyDX88P5TCLL3/LFBoMUIhEsA+jiClKBXivNaJPjJXl26OrICjlYnf/2fOXEmKb16UozMuE1SrgsdUlEZk4G43O1TWi+7wv8M8/9MDI7lkBGaKQ6+IdymEWqX/LFDoMUIhEEluUSswsgabBg6ttxHx7dBcEmS7uu3mXf9MP3PUlFSha+V1EDPHQJbUNVty38jv8729nMfCqDFn3LefFO9TDLFIKzFHoMEAhkkBMUSpv3ddiefr2KGYMf9XuI9g+a7CjbfahIKPpgttibRQZXt9WjrzLUpCSGOsxGJZCzou3v8MsclSADdcCc9GEAQqRRN7KbMvVLe3p26PUMXxXQ0EU2eZ++hPqGy1+7SM1KRZzbroaBp28F29/hlnkTC6Xs/w+yY/TjIlk5m+3tH0VWE/j/BtKjaL2daK61u10Topsp2vqcb5e3MKPzaku3p6+pRtuuSYw1WF9WZmYFWCjC3tQFIqLWIUvMd3XKYmxOOOh693TOL/FKuCTfcdFtSWtlRYPv/89h3NIktZJsXhyTF7AczGkDLOEcmoyhQYDFAXiIlbhTaNWuV28zv6xuWBsN3x35Aze/KrcqW6JWgVM+V2Ox/d5V/lpnK6p99oOXXwMrBYhoD0nahXc1l0Jd0lxGtTU+zdEEq5O1zRg/rqfob64GGAgiR1mYQXY6MMhHoVhF2boyLUmyPqSCreL19m7rwHgjW3lLS7ugmC739P7LDbHxVzbiHvf3iOu0RLMKLgSL47viTmjukZscAIg7IOT1KRY7xt5oLTPHFaAjT7sQfGTnEMx7MIMHbl6rdxN/bWbMyoXQ3MNuP7ZzT6/z1JyXOS+yN7aq51j6nIkByfhTgXgyTF5+OsnJT7P4lHaZ06opyZT8DFA8YPcQzHswgwNb0XVPFV1bRqgpiVp8finrgNMwPZhP39dKfSJsX69z31zUmHQaWE014l6fXL6YO9v+GDvbwD8/4ZO8mil1eBc3aVAtOlnkFqtwr0r9vq8byV95rACbPSRfYjn8ccfh0qlcrp16dLF8XhtbS2KiorQpk0btGrVCuPGjUNlZaXczQi4QAzFsAsz+PxZen19SQWuf3YzJry5E9NX7cPtS76B0ew98CguqxLVts9LKlwONWnUKkzo217UPgKJFWiV4fWJ1+KdKf3x4vieeGdKf2yfNdipYvDiib1g0PnXq6CEz5ymi2I278thBdjIFJAelKuvvhobN268dJCYS4eZMWMG1q1bh/fffx96vR7Tpk3D2LFjsWPHjkA0JSACNRTDLszg87XXyttQjmfinvWv4sP4V/FhxzfiprMdon3tGbrUY9DfyxRgVzNlztTUYf66n0UnUCvlM4cVYKNLQAKUmJgYGAwtF5MymUxYsmQJVq5cicGDBwMAli5diq5du2Lnzp3o379/IJoju0ANxbALM/h86bXyFKCKkX95Gj7ce8zt+9yc0VSLe1fslbUqKEUGsT0GrmbKDM/LxM6yKhSt3IuzF1z/XinxM4cVYKNHQGbxHDhwAFlZWbj88stx++2348iRIwCAPXv2oKGhAQUFBY5tu3Tpgvbt26O4uNjt/urq6mA2m51uoRSooRh2YQafL71Wvpaytxdg69+pjeN9FkPM+j0UXRLj1HiwoDPqGq0+zzjTqFUY0DkNz4zr5ijM1pSSP3PsAdeYnoEpIkfKIHuA0q9fPyxbtgzr16/HokWLUF5ejt/97neorq6G0WhEXFwcUlJSnJ6TkZEBo9F9ZcwFCxZAr9c7btnZ2XI3W5JADsX4Ul2RfGfvtXL38eaqqqsv4/HNP+wL8zLxYMGVkvdDBNgWBXxh4wFMX7UPE97cieuf3ezzdGB+5pBSyT7EM2LECMf/u3fvjn79+qFDhw547733kJCQ4NM+Z8+ejZkzZzp+NpvNIQ1Sendo7bVAlVpl284X7MIMHl/WBPEl8HS14nGDxbcy5ETNP3vEzDjzhJ85pEQBn2ackpKCK6+8EgcPHsTQoUNRX1+Ps2fPOvWiVFZWusxZsdNqtdBqtYFuqmh7Dp/xWgPCKti283VqHhexCh6xiXeO1YDNtUhNinU7i0UFIEOnxT/+2BOnztW1+LDnwn0kNzlqlvAzh5Qm4AHKuXPnUFZWhjvuuAO9e/dGbGwsNm3ahHHjxgEA9u/fjyNHjiA/Pz/QTZENpwMrkz9F85p/g0xL0gIq4NS5OhSXVeFMTT3mrxMXVAgAHv/91RhwRVqLx/yb/UPknpJqlhDJQfYA5eGHH8bo0aPRoUMHHD9+HHPnzoVGo8GECROg1+sxefJkzJw5E6mpqdDpdLj//vuRn58fNjN4AE4HViI5iubZv0GuL6nAwx9873MPR0qi6wJm/s7+IRJjx8FTHKahiCB7gPLbb79hwoQJqKqqQtu2bXH99ddj586daNu2LQDghRdegFqtxrhx41BXV4fhw4fjtddek7sZARVO04GjYVVkfyrBit2XFKbzDZi6Yi9eve0atE7SOs691RrYhfsAIEYFNDICimqvfHnQ8X8uMkrhTCUIQth9nJnNZuj1ephMJuh0upC0wX4hA1wnVgYi+11qsBENqyJbrAKuf3az2wu/PVjcPmuw18DM276kap5InRinwfkwX4COwovUz6No+EJDoSXl+s21eHwU7IqGUoMNOXsVlEzOonm+1jdxp3kiNYMTkkvzGWfuSEmejYYvNBReGKD4IVhT86QGG9G0KrLYRGSj2bYGjqf3yWi6EIgmEskuQUJvnJggPVq+0FB4YYDip0BPzfMl2IimVZHFJiLPX/uT07RgV98MT9fUy94+okBoGpwkxWlQIyJYcRfMR9MXGgovASl1T7Y/+uKyKqzed8znUtSAtGDDztf1ZeRob7B5qwRr17xmiatVp387wx4UCj9ighPAfTDvy2cMUTCwB0UCsQlkco7l+hJsSJ0GHc5jz54qwXrS/JshAKz+/liAWkkUOt5mFbKuEykVAxSRxF7E5R7L9aXmirdp0ACQkhCLRosVL2z4L17cdKDF4+E09uwuYdmb5t8M3VWGJQpXYhb8Y10nUioO8YhgDzqaX/zsF/EXNx7A6n3HsOPgKTz+6U9ux3IB2zd2KcMnvixm52lVZLuzFxpwx1u7XAYn/rQ3VArzMjFnVFefnnuiuhYbS90vVukJR+RJSVISnIsEilnwz5fPGKJgYA9KE66GcAB4TCADgBc2/lfU/n1JTvVlMTvA914Ff9sbbI71cUwXMH/dzz7tY0NpJdb+4NtKsMoP3SiavHpbL6jVKkmzCn39jCEKNAYoF7kawklNisWd/TvIXv1T6liurzVX7NOgd5ZVoWjlXpy94NsQhlLHnuVadM/X4IRISQw6Lfp3auNTIBHsuk5EYjBAgfu8kdM1DVi46aDL5/jjVHUdVu87Jqluiq81VzRqFdRqlc/BCRDcsWcpichcdI/okgl92/vVyxGsuk5EYkV9gBLsBdzUKjgNRUiZLeNrzRVfe0CCvaaQ2ERki1XA459y0T2KfCkJGlxoEFDXaPW6bce0JL+PF+i6TkRSRH2SrNzlzb1pnm/qqh4HYLsI7zh4Cs9/sR/Pf/ELdhw4JSpZ1VU9E396QII19uwtEbnp+Xll8wEYzcocdiKSiwrAM+N64L6BV4janrNsKNJEfQ9KsPIrmi8cZ+eqUuP6kgo8+tGPOHv+0rDMK1+WISUxFs+M7eayt8ViFfDK5oNYuqPcaTgnUx+POaNyvU47bi6YdVCkVLLcUGrECxtdzzwiihSpSbF4+pZuF4ddBCz9utzp86ApJa2eTiSnqF/NuLisChPe3AkAiGmsx51716Lv0VIk1F/A6UQdBLUax5PToKs7D5UgoG3NGZxo1RoCVKjWJiGmoRZ//HETWjXUwqJW4z+X9UD/qjLoa2sQoxJg0rVBfVIrlMSmwFB9Gh3OVEBraUBdTCyqVbFIUAmoV6txPi4RWQlq1Ce1wjfWZGSbTqCd+QTiLA3QQEAjAEAFFQRoAGji4oB6W2l2752/vglW91qg2k9kD3Cb/9/VNq4eb36ft/0JTe5397ymrBcf07jYVt1su1qooYKAOAiO9tZf3E6jiUGMWgXExACNjbZ/BQGIjwd0OqBtW6C8HDh/3vaYTgekpgIVFUBNDWC12u5rbARqa203jca2TWYmcOaM7Wa/v1Ur4PLLbdtfuGB7LCbGdpzBg23HOXwY2LHD9jnVujUwdChQVmZrV5s2wGWXAZ07A127Am+/DRw6BHToAEycaDvGV1/ZXvx11wElJbZ91dTYjmG1AlVVQGIikJVl2yY72/bvV18BW7bYnnvDDYBaDZw4YXsd110HfP217XVnZgLduwO//z2wf7/tXI0fDxQW2rZbtMi2r6QkoEcPwGCwHbNtW9v/Adt+09Mv/T8zE/jd72w/f/WV7ThNH3e1rabJu19fD7z2mu08deoE3HcfEBcHWCyX9ufqed5Ifb6/x/NA0vVbCEMmk0kAIJhMJr/31WixCtc88YWwqO9YoREqQbD9+fDGG2+88RZuN7Xa8+MaTeDb0KaN7SZm23btBOHDD20Xo0ceadk+jUYQxoyxbefued58+KG050vdXiIp12/IcsQgkzNAEQRBODhpqmAFBGuo/7h444033niLnptKZbuNGePb87wFDR9+aNtO7POlbu8DKdfvqB/iQX09kJAAwWplVVAiIgoulcoWBvjyvHbtbEN3roZfLBagY0fgt9/EPV/q9j6Scv2O+lk8eO01gMEJERGFgq99BIIAHD16KVenua++ch9suHq+1O2DgAFKWVmoW0BEROSbCjeVsN3d7247qdsHAQOUTp1C3QIiIiLfZLopBeHufnfbSd0+CJiDcjEHBVZOdiUioiALdA7KsWOu9+8uB0Xs9j5iDooUcXHAQw+FuhVERBRtVBezH8eM8e15Cxe6DxY0GuDFF5239/R8qdsHAQMUAHjuOeCRR2wFfYiIKDx5+wwPxsW1TRvbTYx27YAPPgA++cR2DWrePo3GFry0a+f6eWPHet7/2LG27S67TNzzpW4fYBziaaq+HpaXXsZXb30IVc15vyrJJtfWQCMIMMbrcT4hEb/p071WktVaGlCtTcRRXbrHSrJqAOehRhKCN/tIgOtKmERK1vT31t3vsP1+V483v8/b/oQm99sfc3XJtDb5V0DLSrICbFVitU0fi421db03Nl7aOCbGNjytVtu+5bKSLCvJuhKmlWQZoLhgX7gOQNSumJuSEAOoVG7X/yCKVioADwy5Ai9uOuh127cn98OAzmkt7m+6xIYn70zpz9WFKaIwB8VPhXmZWDSxFwz66F0d9OyFRpw934AReRkozMtoMSRJFI1SEmKxaGIvUSuLA0Dxr6dc3t83JxWZ+ni3vZIq2Bbs5AKAFM2ifjVjd2yriBqwq/w0NpQa8daOQ6FuUkh8XlIZ6iYQKcart/fCgCvSUHLMLPIZLUMQi1XArvLTGJlnwBIXnyv2Z8wdnQuNmt8MKHoxQPFAo1Yhv1Mb5Hdqg2s7tMa0d76DyC9ORBRBVAAM+nj0v9w23JLfqQ1e+dL7EE/z4Zn1JRWYt6YUFaZax31qFZw+Vwz6eMwdnYvCvODVmyBSIgYoIrVO0jI4IYpSAoDf98h09Gj0v7wNUhJjPeZotU6MdQQ0wKXctuYfI/bPlbuv64BhV2eib04qe06IwBwU0U5U13rfiIgi1hvbyrG+xFbmW6NW4Zmx3Txuv2BsN0egYbEKmLem1GPS/fLiwzhTU8fghOgiBihNWKwCisuq8PF3x7Dkq1/x8d7fUFxWBYtVQHpy9CbMEpHNvDWljgTZwrxMLJ7YCwad82dDpj4eiyf2chqi2VV+2mlYxxWrANy38jtHEEQU7TjEc5GrsWE7g06LP/XJhj4hFqYLnHZLFI0EABWmWuwqP+3ILWmaTH+iuhbpyfEuh2ik9MDOW1OKobkG9qRQ1GOAAvdjw3ZGc52omgdEFPmaBxv2ZHpPpPTANg+CiKJV1A/xiBkbJiKy82W41173RCzmvBGFOEB59dVX0bFjR8THx6Nfv37YtWtX0NsgZmyYiKJDalKcxyUdfC2eplGrMHd0rujtmfNGFMIA5d1338XMmTMxd+5c7N27Fz169MDw4cNx4sSJoLaD31SIyF659ckxeR63azrVWKrCvEy8dlsveHo6K8gSXRKyAOWf//wnpkyZgrvvvhu5ublYvHgxEhMT8dZbbwW1HfymQhTdmlZuHdk9E/fckON226ZTjX0xsnsmXplwjdd2MEGWKEQBSn19Pfbs2YOCgoJLDVGrUVBQgOLi4hbb19XVwWw2O93k4m1NDCLyT3yMCi/+qSfmjOoa0nZk6uPxvzfktMgFMejjsejitGCLVcCn33sOQJpONfbFyO5ZWDyxl8d2EFGIZvGcOnUKFosFGRkZTvdnZGTgl19+abH9ggULMG/evIC0xT42bF+9mIjkVdso4MF39+HuAR0DfqxMfTyev7UHTtXUIS1JC6iAU+fqnKb//qWwq9tpwd5y0lxNNfaF2OnJRNEsLKYZz549GzNnznT8bDabkZ2dLdv+7asXP/5pKYxmaTkpKoAzgIhEWL3veED3r4JteGRA5zSP23maFiw2J02O3DUx05OJollIApS0tDRoNBpUVjqvlFtZWQmDwdBie61WC61WG9A22b/RvLL5AF7YeMDjtpkXF/MC4La4GxFdIgCoqqlHalIsztQ0yB7Ut06MxYKx3fweHhGbk8bcNaLAC0mAEhcXh969e2PTpk24+eabAQBWqxWbNm3CtGnTQtEkALZvNNMLrsRVhuQWgUebpDiM6ZmFobkGp67Ypt20h06dx9Kvy10uIKaNUQOCgDqL+4/m+Bg14uM0HhcgIwpnt/S8DG/tOCSp59HTF4KUhFjcPaAjpg3uLMvwiD0nzWiqddk++6rGnGVDFHgqQRBCMkLx7rvvYtKkSXj99dfRt29fLFy4EO+99x5++eWXFrkpzZnNZuj1ephMJuh0uoC0z2IVfBoftlgF7Py1Cl+XncKxMxeQlRKPAZ3aov/Frtydv1ahuKwKgiBAlxAL84UGqFRA/uVpjm2aHrd3h9aY+d4+rPuhosUHZiutBufqLHK/dKKAeWdKf5gu1LcINDL18ZgzqitaJ2lhNNfi9Lk6pCbFwaBPcPrb8/XvUgp7ZWnAOYiyH4WJrES+k3L9DlmAAgCvvPIK/v73v8NoNKJnz5546aWX0K9fP6/PC0aAojT1jVb8u/gQDp8+jw6pibgjvyPiYtSO+w9VnYcgWHHs7Hls3V8Fq5f9qQCoVYCHDh2KAlN+l4M3vyqX/Dy1yra4nZ1Bp0VtoxWm866Hb+w9D9tnDYZGrQpKoOEPV2tz2XtyGJwQ+S5sAhRfRWOAIkXzD//eHVpj96HTKC6rAiB47K1ZtOUglu44hLNNFkVspVXj7vwcCCoVGq0WnKu19dqoVEDPdinIap2IntkpWPnNYRw+fR7ZrRPQxaDD6fP1jouPxSpg+deHsPPXKhyoNON0TT0sAnChwVsoJV6WLhbtW8dj5+HqFo+5GlKIj1WhdXwsEuJUOF3TAFOt1eOwQ6wKaAjAX0vbpFg0CsCZ887nvFPbVthfeQ61Hs5RQqwK7VOTIAgCGixWHD97AWI61ZpebF1djJO0GlzfqQ3SkrVQq2zH6GJIdrynvTu0xp7DZ5wCjA2lxojqeVB6EEUUjhigkF+C+cHc/Fj2QKe8qgYqANdkt0ZmSkKLC2LTgKhpj5K79gMQ9ZrszzWaLuB0TT1SW2lh0F3avr7RiuVfH8LuQ6eREKvG1Zl6pCVrUVVTh1Pn6lByzIy6RgtUKhUOVp7DmSaBXkpCLO7o3x5nzjfg8Onz6NgmEX8dmYuEOI3bc+5oz8Vhj5SEWJy90NCiXVLPp7fn+fqes+eBiDxhgEKkANH6DTxaXzcReSfl+h0WdVCIwlG01rmI1tdNRPIK6WrGRERERK4wQCEiIiLFYYBCREREisMAhYiIiBSHAQoREREpDgMUIiIiUhwGKERERKQ4DFCIiIhIcRigEBERkeKEZSVZe3V+s9kc4pYQERGRWPbrtphVdsIyQKmutq1Wm52dHeKWEBERkVTV1dXQ6/UetwnLxQKtViuOHz+O5ORkqFSBXYTMbDYjOzsbR48e5cKEfuB5lAfPozx4HuXB8+i/aDuHgiCguroaWVlZUKs9Z5mEZQ+KWq1Gu3btgnpMnU4XFb88gcbzKA+eR3nwPMqD59F/0XQOvfWc2DFJloiIiBSHAQoREREpDgMUL7RaLebOnQutVhvqpoQ1nkd58DzKg+dRHjyP/uM5dC8sk2SJiIgosrEHhYiIiBSHAQoREREpDgMUIiIiUhwGKERERKQ4DFBEOnToECZPnoycnBwkJCSgU6dOmDt3Lurr60PdNMV79dVX0bFjR8THx6Nfv37YtWtXqJsUVhYsWIA+ffogOTkZ6enpuPnmm7F///5QNyvsPfPMM1CpVHjwwQdD3ZSwc+zYMUycOBFt2rRBQkICunXrhm+//TbUzQorFosFc+bMcbqmzJ8/X9QaNdEiLCvJhsIvv/wCq9WK119/HVdccQVKSkowZcoU1NTU4Pnnnw918xTr3XffxcyZM7F48WL069cPCxcuxPDhw7F//36kp6eHunlhYevWrSgqKkKfPn3Q2NiIv/71rxg2bBhKS0uRlJQU6uaFpd27d+P1119H9+7dQ92UsHPmzBkMGDAAgwYNwueff462bdviwIEDaN26daibFlaeffZZLFq0CMuXL8fVV1+Nb7/9FnfffTf0ej0eeOCBUDdPETjN2A9///vfsWjRIvz666+hbopi9evXD3369MErr7wCwLaOUnZ2Nu6//348+uijIW5deDp58iTS09OxdetW3HDDDaFuTtg5d+4cevXqhddeew1PPvkkevbsiYULF4a6WWHj0UcfxY4dO/DVV1+Fuilh7aabbkJGRgaWLFniuG/cuHFISEjAihUrQtgy5eAQjx9MJhNSU1ND3QzFqq+vx549e1BQUOC4T61Wo6CgAMXFxSFsWXgzmUwAwN89HxUVFWHUqFFOv5ck3qeffoprr70Wf/jDH5Ceno5rrrkGb775ZqibFXauu+46bNq0Cf/9738BAN9//z22b9+OESNGhLhlysEhHh8dPHgQL7/8Mod3PDh16hQsFgsyMjKc7s/IyMAvv/wSolaFN6vVigcffBADBgxAXl5eqJsTdlatWoW9e/di9+7doW5K2Pr111+xaNEizJw5E3/961+xe/duPPDAA4iLi8OkSZNC3byw8eijj8JsNqNLly7QaDSwWCx46qmncPvtt4e6aYoR9T0ojz76KFQqlcdb84vpsWPHUFhYiD/84Q+YMmVKiFpO0aioqAglJSVYtWpVqJsSdo4ePYrp06fj7bffRnx8fKibE7asVit69eqFp59+Gtdccw3uueceTJkyBYsXLw5108LKe++9h7fffhsrV67E3r17sXz5cjz//PNYvnx5qJumGFHfg/LQQw/hrrvu8rjN5Zdf7vj/8ePHMWjQIFx33XV44403Aty68JaWlgaNRoPKykqn+ysrK2EwGELUqvA1bdo0rF27Ftu2bUO7du1C3Zyws2fPHpw4cQK9evVy3GexWLBt2za88sorqKurg0ajCWELw0NmZiZyc3Od7uvatSs+/PDDELUoPD3yyCN49NFHMX78eABAt27dcPjwYSxYsIA9URdFfYDStm1btG3bVtS2x44dw6BBg9C7d28sXboUanXUd0B5FBcXh969e2PTpk24+eabAdi+fW3atAnTpk0LbePCiCAIuP/++/Hxxx9jy5YtyMnJCXWTwtKQIUPw448/Ot139913o0uXLpg1axaDE5EGDBjQYpr7f//7X3To0CFELQpP58+fb3EN0Wg0sFqtIWqR8kR9gCLWsWPHMHDgQHTo0AHPP/88Tp486XiMvQHuzZw5E5MmTcK1116Lvn37YuHChaipqcHdd98d6qaFjaKiIqxcuRKrV69GcnIyjEYjAECv1yMhISHErQsfycnJLfJ2kpKS0KZNG+bzSDBjxgxcd911ePrpp/HHP/4Ru3btwhtvvMEeZYlGjx6Np556Cu3bt8fVV1+N7777Dv/85z/x5z//OdRNUw6BRFm6dKkAwOWNPHv55ZeF9u3bC3FxcULfvn2FnTt3hrpJYcXd793SpUtD3bSwd+ONNwrTp08PdTPCzpo1a4S8vDxBq9UKXbp0Ed54441QNynsmM1mYfr06UL79u2F+Ph44fLLLxf+7//+T6irqwt10xSDdVCIiIhIcZhEQURERIrDAIWIiIgUhwEKERERKQ4DFCIiIlIcBihERESkOAxQiIiISHEYoBAREZHiMEAhIiIixWGAQkRERIrDAIWIiIgUhwEKERERKQ4DFCIiIlKc/w/+eXnOoS4TsAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}